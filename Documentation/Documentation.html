<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=lhDjYqiy3mZ0x6ROQEUoUw');.lst-kix_btbuo2ptomm8-1>li:before{content:"\0025cb  "}ul.lst-kix_btbuo2ptomm8-2{list-style-type:none}ul.lst-kix_btbuo2ptomm8-3{list-style-type:none}ul.lst-kix_btbuo2ptomm8-0{list-style-type:none}ul.lst-kix_btbuo2ptomm8-1{list-style-type:none}.lst-kix_btbuo2ptomm8-3>li:before{content:"\0025cf  "}.lst-kix_btbuo2ptomm8-0>li:before{content:"\0025cf  "}.lst-kix_btbuo2ptomm8-4>li:before{content:"\0025cb  "}ul.lst-kix_btbuo2ptomm8-8{list-style-type:none}ul.lst-kix_btbuo2ptomm8-6{list-style-type:none}ul.lst-kix_btbuo2ptomm8-7{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_btbuo2ptomm8-2>li:before{content:"\0025a0  "}ul.lst-kix_btbuo2ptomm8-4{list-style-type:none}ul.lst-kix_btbuo2ptomm8-5{list-style-type:none}.lst-kix_btbuo2ptomm8-7>li:before{content:"\0025cb  "}.lst-kix_btbuo2ptomm8-8>li:before{content:"\0025a0  "}.lst-kix_btbuo2ptomm8-5>li:before{content:"\0025a0  "}.lst-kix_btbuo2ptomm8-6>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c52{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;background-color:#1e1e1e;border-left-style:solid;border-bottom-width:0pt;width:451.4pt;border-top-color:#000000;border-bottom-style:solid}.c3{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;background-color:#333333;border-left-style:solid;border-bottom-width:0pt;width:451.4pt;border-top-color:#000000;border-bottom-style:solid}.c64{-webkit-text-decoration-skip:none;color:#134f5c;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:20pt;font-family:"Arial";font-style:normal}.c7{background-color:#1e1e1e;color:#57a64a;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Consolas";font-style:italic}.c0{background-color:#1e1e1e;color:#57a64a;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Consolas";font-style:italic}.c1{color:#212121;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c14{color:#38761d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c4{color:#38761d;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c39{background-color:#1e1e1e;color:#57a64a;font-weight:400;font-size:10.5pt;font-family:"Consolas";font-style:italic}.c74{color:#212121;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:italic}.c48{padding-top:20pt;padding-bottom:6pt;line-height:1.15;text-align:left;height:20pt}.c29{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c19{background-color:#1e1e1e;font-size:10.5pt;font-family:"Consolas";color:#569cd6;font-weight:400}.c21{background-color:#333333;font-size:9.5pt;font-family:"Consolas";color:#ffffff;font-weight:400}.c72{background-color:#333333;font-size:10.5pt;font-family:"Consolas";color:#ffffaa;font-weight:400}.c69{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#ffffaa;font-weight:400}.c53{background-color:#333333;font-size:9.5pt;font-family:"Consolas";color:#888888;font-weight:400}.c18{background-color:#1e1e1e;font-size:10.5pt;font-family:"Consolas";color:#dcdcdc;font-weight:400}.c5{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#ffffff;font-weight:400}.c30{background-color:#333333;font-size:10.5pt;font-family:"Consolas";color:#ffffff;font-weight:400}.c6{background-color:#333333;font-size:9.5pt;font-family:"Consolas";color:#a2fca2;font-weight:400}.c38{background-color:#333333;font-size:10.5pt;font-family:"Consolas";color:#fcc28c;font-weight:400}.c23{padding-top:16pt;padding-bottom:4pt;line-height:1.15;text-align:left;height:14pt}.c2{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#fcc28c;font-weight:400}.c24{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#888888;font-weight:400}.c41{background-color:#1e1e1e;font-size:10.5pt;font-family:"Consolas";color:#b8d7a3;font-weight:400}.c15{background-color:#1e1e1e;font-size:10.5pt;font-family:"Consolas";color:#d69d85;font-weight:400}.c44{background-color:#333333;font-size:9.5pt;font-family:"Consolas";color:#fcc28c;font-weight:400}.c70{background-color:#333333;font-size:10.5pt;font-family:"Consolas";color:#a2fca2;font-weight:400}.c49{background-color:#1e1e1e;color:#57a64a;font-weight:400;font-family:"Consolas";font-style:italic}.c13{background-color:#1e1e1e;font-size:10pt;font-family:"Consolas";color:#dcdcdc;font-weight:400}.c12{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#d36363;font-weight:400}.c45{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#a2fca2;font-weight:400}.c56{padding-top:0pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c40{color:#000000;font-weight:400;font-size:9pt;font-family:"Arial"}.c31{color:#cc0000;font-weight:400;font-size:14pt;font-family:"Arial"}.c42{color:#000000;font-weight:700;font-size:11pt;font-family:"Arial"}.c59{color:#434343;font-weight:400;font-size:14pt;font-family:"Arial"}.c61{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c35{background-color:#1e1e1e;font-family:"Consolas";color:#569cd6;font-weight:400}.c63{color:#bbdaff;font-weight:400;font-size:10.5pt;font-family:"Consolas"}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c36{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c26{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c51{padding-top:0pt;padding-bottom:3pt;line-height:1.15;text-align:left}.c47{color:#c9d1d9;font-weight:400;font-size:10.5pt;font-family:"Arial"}.c17{background-color:#1e1e1e;font-family:"Consolas";color:#d69d85;font-weight:400}.c46{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:center}.c32{padding-top:16pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c65{padding-top:14pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c34{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c37{border-spacing:0;border-collapse:collapse;margin-right:auto}.c75{padding-top:24pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c27{background-color:#1e1e1e;font-family:"Consolas";color:#dcdcdc;font-weight:400}.c55{color:#000000;font-weight:400;font-family:"Arial"}.c67{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c22{text-decoration:none;vertical-align:baseline;font-style:normal}.c25{font-weight:700;font-style:italic}.c28{color:inherit;text-decoration:inherit}.c66{margin-left:36pt;padding-left:0pt}.c10{orphans:2;widows:2}.c71{padding:0;margin:0}.c58{color:#000000;text-decoration:none}.c20{page-break-after:avoid}.c11{height:11pt}.c54{vertical-align:super}.c60{font-size:9.5pt}.c33{font-size:10pt}.c73{font-size:13pt}.c57{height:216.8pt}.c68{height:16pt}.c16{height:0pt}.c43{margin-left:18pt}.c50{font-weight:700}.c62{font-size:26pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:0pt;color:#38761d;font-weight:700;font-size:16pt;padding-bottom:0pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#cc0000;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#38761d;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c67 doc-content"><p class="c10 c20 c51 title" id="h.caspr5vic333"><span class="c55 c22 c62">READ ME</span></p><h2 class="c8 c10 c20 c68" id="h.omeubuhxrxbw"><span class="c4"></span></h2><p class="c34"><span class="c42 c22"><a class="c28" href="#h.bbkfn1omw6w2">What is it?:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2</a></span></p><p class="c34"><span class="c42 c22"><a class="c28" href="#h.ub1lvyokxtm">Prerequisites:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2</a></span></p><p class="c34"><span class="c42 c22"><a class="c28" href="#h.b53ba3c51zjw">Structure:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2</a></span></p><p class="c34 c43"><span class="c9"><a class="c28" href="#h.q7ze64iletz1">Functions used throughout:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3</a></span></p><p class="c34 c43"><span class="c9"><a class="c28" href="#h.9yfmc6qy2clu">GWAS:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5</a></span></p><p class="c34 c43"><span class="c9"><a class="c28" href="#h.cdq9271tst5s">Variant frequency data by population:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6</a></span></p><p class="c34 c43"><span class="c9"><a class="c28" href="#h.o8avd7mg5nfg">Functional information and Gene Ontology:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7</a></span></p><p class="c29"><span class="c9"><a class="c28" href="#h.x1xaobp5m3xd">Collecting data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7</a></span></p><p class="c29"><span class="c9"><a class="c28" href="#h.k6rssmwl5vmz">Trimming data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9</a></span></p><p class="c29"><span class="c58"><a class="c28" href="#h.2qyx09azowbm">Outputting Functional information and Gene Ontology&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10</a></span></p><p class="c34 c43"><span class="c9"><a class="c28" href="#h.x88z4oadkpbf">Linkage Disequilibrium:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11</a></span></p><p class="c29"><span class="c9"><a class="c28" href="#h.9g8mzey51un2">Collecting data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11</a></span></p><p class="c29"><span class="c9"><a class="c28" href="#h.2wm5ip20i7c1">Outputting LD results:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;13</a></span></p><p class="c29"><span class="c9"><a class="c28" href="#h.hebu2brlzmob">LD heatmap plots:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14</a></span></p><p class="c34 c43"><span class="c9"><a class="c28" href="#h.5zfnv0ig9q0m">Manhattan Plot:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17</a></span></p><p class="c34 c43"><span class="c9"><a class="c28" href="#h.2svyklxr9h57">User Feedback:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20</a></span></p><p class="c34"><span class="c22 c42"><a class="c28" href="#h.hwsiupgp3f8i">Flask:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20</a></span></p><p class="c34"><span class="c42 c22"><a class="c28" href="#h.147jjkfr6rv5">Navigation:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20</a></span></p><p class="c34"><span class="c42 c22"><a class="c28" href="#h.fysf9tqgmmsh">Citation:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20</a></span></p><hr style="page-break-before:always;display:none;"><h2 class="c8 c10 c20 c68" id="h.uwl9ldm36wt9"><span class="c4"></span></h2><h2 class="c8 c10 c20" id="h.bbkfn1omw6w2"><span class="c4">What is it?:</span></h2><p class="c8 c10"><span class="c9">This web application prototype is designed to retrieve information on Single Nucleotide Polymorphisms (SNPs) seen in Type 1 Diabetes patients identified by Genome wide association studies (GWAS). The database will use information from the GWAS catalogue, along with population data from Ensembl and the 1000 Genomes Project and functional information and Gene Ontology information obtained through Ensembl&rsquo;s VEP tool which is all is retrievable through a user friendly interface through the input of an rsID, Chromosome position or a Gene name. The site also allows the user to calculate Linkage Disequilibrium (LD) of SNPs selected for each population producing a text file containing the LD values and plot these values as a LD heatmap. The user is also able to enter multiple SNPs and return a Manhattan plot of p-values.</span></p><p class="c8 c10 c11"><span class="c9"></span></p><h2 class="c8 c10 c20" id="h.ub1lvyokxtm"><span class="c4">Prerequisites:</span></h2><p class="c8 c10"><span class="c9">Python 3.10.10</span></p><p class="c8 c10"><span class="c9">Flask &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.2.3</span></p><p class="c8 c10"><span class="c9">Flask-WTF &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.1.1</span></p><p class="c8 c10"><span class="c9">bokeh &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.0.3</span></p><p class="c8 c10"><span class="c9">Jinja2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3.1.2</span></p><p class="c8 c10"><span class="c9">pandas &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1.5.3</span></p><p class="c8 c10"><span class="c9">matplotlib &nbsp; &nbsp; &nbsp; &nbsp; 3.7.0</span></p><p class="c8 c10"><span class="c9">numpy &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1.24.2</span></p><h2 class="c8 c10 c20" id="h.b53ba3c51zjw"><span class="c4">Structure:</span></h2><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 579.50px; height: 396.60px;"><img alt="" src="images/image8.png" style="width: 579.50px; height: 396.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c32 c10 c20" id="h.q7ze64iletz1"><span class="c31 c22">Functions used throughout:</span></h3><p class="c8 c10"><span>A number of functions were used throughout the code such as </span><span class="c50">removeDupeSNP():</span><span>&nbsp;(seen below) this function removes duplicate SNPs from a pandas dataframe leaving only the SNP with the greatest P-value. This was done to provide a less cluttered output and overall more aesthetically pleasing table that is more &ldquo;human friendly&quot;, with the exception of the Manhattan plot which uses the different P-values of the duplicate SNPs.</span></p><p class="c8 c10"><span class="c9">The code first identifies the duplicate rows, and creates a dictionary to store the index and p-value of each duplicate row. </span></p><p class="c8 c10 c11"><span class="c9"></span></p><a id="t.61508210b672157dd5691ded11ca7907c43671a9"></a><a id="t.0"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c35">def</span><span class="c27">&nbsp;removeDupeSNP(dataframe): &nbsp; &nbsp; <br></span><span class="c49"># Removes duplicates from a pandas dataframe, leaving only greatest p-value</span><span class="c27"><br> &nbsp; dataframe.reset_index(drop=</span><span class="c35">True</span><span class="c27">) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span><span class="c49"># Resets index back to 0</span><span class="c27"><br> &nbsp; dupeList = dataframe.duplicated(subset=</span><span class="c17">&#39;snps&#39;</span><span class="c27">,keep=</span><span class="c35">False</span><span class="c27">) &nbsp; <br></span><span class="c49"># Get list of duplicate values</span><span class="c27"><br> &nbsp; dupes=dataframe[dupeList] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c7"># Select dataframe using above list</span></p><p class="c8"><span class="c18">&nbsp; &nbsp;dupesDict={}<br> &nbsp; </span><span class="c19">for</span><span class="c18">&nbsp;index,row </span><span class="c19">in</span><span class="c18">&nbsp;dupes.iterrows(): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span><span class="c39"># Iterate through df of duplicates, one row at a time</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; rsVal=row[</span><span class="c15">&quot;snps&quot;</span><span class="c18">] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c39"># SNP name (rs value)</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; snpTuple=(index,row[</span><span class="c15">&quot;p_value&quot;</span><span class="c18">]) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c39"># Tuple containing index and p-val</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; </span><span class="c19">if</span><span class="c18">&nbsp;rsVal </span><span class="c19">in</span><span class="c18">&nbsp;dupesDict: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span><span class="c39"># If it&#39;s seen the snp before,</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dictList=dupesDict[rsVal] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c39"># go to the value for the snp,</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dictList.append(snpTuple) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c39"># and add the index/ p-val tuple.</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; </span><span class="c19">else</span><span class="c18">: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c39"># If it hasn&#39;t seen the snp before,</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dupesDict.update({rsVal:[snpTuple]}) &nbsp; &nbsp;<br></span><span class="c0"># create a listing for it.</span></p><p class="c8"><span class="c18">&nbsp; &nbsp;naughtyList=[] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c0"># List of lists of (index, p-val) we want to drop</span></p></td></tr></table><p class="c8"><span class="c18"><br></span><span class="c9">The duplicate rows are then sorted by p-value, keeping only the row with the greatest p-value, and creates a list of the indices of the rows to be dropped. </span></p><p class="c8 c11"><span class="c9"></span></p><a id="t.b2c7c35a59cf52de498a21cdb6e81c871dfe66ad"></a><a id="t.1"></a><table class="c37"><tr class="c57"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c18">&nbsp; &nbsp;</span><span class="c19">for</span><span class="c18">&nbsp;i </span><span class="c19">in</span><span class="c18">&nbsp;dupesDict:<br> &nbsp; &nbsp; &nbsp; snp = dupesDict[i] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c39"># Get list of (index, pVal)</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; sortByP=sorted(snp,key=</span><span class="c19">lambda</span><span class="c18">&nbsp;x: </span><span class="c41">0</span><span class="c18">-x[</span><span class="c41">1</span><span class="c18">]) &nbsp; &nbsp;</span><span class="c39"># Sort by p-value</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; sortByP=sortByP[</span><span class="c41">1</span><span class="c18">:] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c39"># Select all but greatest p value</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; naughtyList.append(sortByP)<br><br><br> &nbsp; dropList=[] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c39"># List of indices for rows we want to drop</span><span class="c18"><br> &nbsp; </span><span class="c19">for</span><span class="c18">&nbsp;i </span><span class="c19">in</span><span class="c18">&nbsp;naughtyList: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c39"># Enter first list</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; </span><span class="c19">for</span><span class="c18">&nbsp;j </span><span class="c19">in</span><span class="c18">&nbsp;i: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c39"># Enter second list</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dropList.append(j[</span><span class="c41">0</span><span class="c18">]) &nbsp; &nbsp; &nbsp; </span><span class="c39"># Add the index from each tuple</span><span class="c18"><br></span></p></td></tr></table><p class="c8"><span class="c18"><br></span><span>Finally, the function returns the input DataFrame with the identified duplicate rows removed.</span><span class="c18"><br></span></p><a id="t.bb66a4ef8a364ea46d977fb405f81bdff62ca778"></a><a id="t.2"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c18">&nbsp; &nbsp;</span><span class="c19">return</span><span class="c18">(dataframe.drop(dropList)) &nbsp; &nbsp;</span><span class="c39"># Return dataframe without duplicate values</span></p></td></tr></table><p class="c8 c11"><span class="c18 c22"></span></p><p class="c8 c10"><span>Another function made and used throughout was </span><span class="c50">removeDupeGeneMap(GeneMap): </span><span>this f</span><span class="c9"><br></span></p><a id="t.59b2d73cce28e0f846926b398a5396f1238f472c"></a><a id="t.3"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c19">def</span><span class="c18">&nbsp;removeDupeGeneMap(GeneMap):<br> &nbsp; </span><span class="c19">try</span><span class="c18">:<br> &nbsp; &nbsp; &nbsp; GeneMap=GeneMap.split(</span><span class="c15">&#39;, &#39;</span><span class="c18">)<br> &nbsp; &nbsp; &nbsp; uniques=</span><span class="c15">&quot;&quot;</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; </span><span class="c19">for</span><span class="c18">&nbsp;item </span><span class="c19">in</span><span class="c18">&nbsp;GeneMap:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c19">if</span><span class="c18">&nbsp;item </span><span class="c19">not</span><span class="c18">&nbsp;</span><span class="c19">in</span><span class="c18">&nbsp;uniques: </span><span class="c39"># If the item hasn&#39;t been seen before,</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; uniques+=(item) &nbsp; &nbsp; </span><span class="c39"># add it to the list.</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; uniques+=(</span><span class="c15">&quot;, &quot;</span><span class="c18">) &nbsp; &nbsp; </span><span class="c39"># Also add &#39; ,&#39;</span><span class="c18"><br> &nbsp; &nbsp; &nbsp; </span><span class="c19">return</span><span class="c18">&nbsp;(uniques[:</span><span class="c41">-2</span><span class="c18">]) &nbsp; &nbsp; &nbsp; </span><span class="c39"># Remove last &#39; ,&#39;</span><span class="c18"><br> &nbsp; </span><span class="c19">except</span><span class="c18">:<br> &nbsp; &nbsp; &nbsp; </span><span class="c19">return</span><span class="c18">&nbsp;(</span><span class="c15">&quot;Data unavailable&quot;</span><span class="c18">) &nbsp; &nbsp; </span><span class="c39"># Return this if geneMap is empty</span></p></td></tr></table><hr style="page-break-before:always;display:none;"><h3 class="c23 c10 c20" id="h.44vzlyaxdo3p"><span class="c31 c22"></span></h3><h3 class="c32 c10 c20" id="h.9yfmc6qy2clu"><span class="c31 c22">GWAS:</span></h3><p class="c8 c10"><span class="c9">This information was downloaded from the GWAS catalogue where a TSV file was downloaded and then trimmed using the following code;</span></p><a id="t.b1af7d7bc328582cfe763ff0e0cbb9d8bcbeb335"></a><a id="t.4"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c13">fileIn = getPath(</span><span class="c17 c33">&#39;gwas_catalog_v1.0-associations_e108_r2023-01-14.tsv&#39;</span><span class="c13">) </span><span class="c49 c33"># https://www.ebi.ac.uk/gwas/docs/file-downloads</span><span class="c13"><br>fileOut = getPath(</span><span class="c17 c33">&#39;gwas_trimmed.tsv&#39;</span><span class="c13">)<br><br>data = pd.read_csv(fileIn, sep=</span><span class="c17 c33">&#39;\t&#39;</span><span class="c13">, low_memory=</span><span class="c35 c33">False</span><span class="c13">) &nbsp; &nbsp;</span><span class="c49 c33"># Reads gwas tsv</span><span class="c13"><br>data=removeSpecial(data) &nbsp; &nbsp;</span><span class="c33 c49"># removes special characters in column names</span></p></td></tr></table><p class="c8 c10"><span class="c9">This code uses pandas to open the TSV file, creates a dataframe called data, and removes any special characters from the column names, as SQL does not interact with special characters very well.</span></p><a id="t.638fb5b406f87bace7364f8ea9275d9a6c56f7f3"></a><a id="t.5"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c27">data=data.query(</span><span class="c17">&quot;disease_trait==&#39;Type 1 diabetes&#39; or study.str.contains(&#39;type 1 diabetes&#39;)&quot;</span><span class="c27">)<br>data = data.loc[data.snps.str.contains(</span><span class="c17">r&#39;rs[0-9]+&#39;</span><span class="c27">)] &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c49"># get only snps with rsids</span><span class="c27"><br></span><span class="c49">#data = data.loc[data[&#39;CHR_ID&#39;]==&#39;6&#39;] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Select only rows for chromosome 6</span></p></td></tr></table><p class="c8 c10"><span>The data frame is then filtered further so that it only has data that references T1D in the &ldquo;disease_trait&rdquo; column so that only T1D data remains in the dataframe. Next all SNPs that don&#39;t have rsIDs are removed, as some cells had incompatible data in this column.</span></p><a id="t.132f647eb56f42c8d7dbb1fffca78301f723bdb3"></a><a id="t.6"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c13">data = data[[</span><span class="c17 c33">&quot;snps&quot;</span><span class="c13">,</span><span class="c17 c33">&quot;region&quot;</span><span class="c13">,</span><span class="c17 c33">&quot;chr_pos&quot;</span><span class="c13">,</span><span class="c17 c33">&quot;chr_id&quot;</span><span class="c13">,</span><span class="c17 c33">&quot;p_value&quot;</span><span class="c13">,</span><span class="c17 c33">&quot;mapped_gene&quot;</span><span class="c13">]]</span></p></td></tr></table><p class="c8 c10"><span>The da</span><span class="c9">taframe is then trimmed again so that it contains only the columns of interest &nbsp; &quot;snps&quot;, &quot;region&quot;, &quot;chr_pos&quot;, &quot;chr_id&quot;, &quot;p_value&quot;, &quot;mapped_gene&quot;. </span></p><p class="c8 c10 c11"><span class="c9"></span></p><a id="t.8adbe8fae2f67de92759b4b70695249a62c02e6d"></a><a id="t.7"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c27">data=removeDupeSNP(data) &nbsp; &nbsp;</span><span class="c49"># Remove duplicates (leaving the entry with largest p value)</span><span class="c27"><br>newCol=[removeDupeGeneMap(r[</span><span class="c17">&quot;mapped_gene&quot;</span><span class="c27">]) </span><span class="c35">for</span><span class="c27">&nbsp;i, r </span><span class="c35">in</span><span class="c27">&nbsp;data.iterrows()]</span></p><p class="c8"><span class="c27">data[</span><span class="c17">&quot;mapped_gene&quot;</span><span class="c27">]=newCol</span></p></td></tr></table><p class="c8 c10"><span class="c9">Duplicate mapped_gene information is then removed.</span></p><p class="c8 c10 c11"><span class="c9"></span></p><a id="t.efbc0c483d3f71e27abdb0a27605856edba6693c"></a><a id="t.8"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c13">data.rename(columns = {</span><span class="c17 c33">&#39;snps&#39;</span><span class="c13">:</span><span class="c17 c33">&#39;rsid&#39;</span><span class="c13">}, inplace = </span><span class="c35 c33">True</span><span class="c13">)<br><br></span><span class="c49 c33"># if os.path.exists(fileOut): # If the file exists,</span><span class="c13"><br></span><span class="c49 c33"># &nbsp; &nbsp; os.remove(fileOut) &nbsp; &nbsp; # delete it.</span><span class="c13"><br>data.to_csv(fileOut, sep=</span><span class="c17 c33">&#39;\t&#39;</span><span class="c13">, index=</span><span class="c35 c33">False</span><span class="c13">)</span></p></td></tr></table><p class="c8 c10"><span>Next the column &#39;snps&#39; was renamed to &#39;rsid&#39; and made the change directly to the dataframe by setting inplace=True.</span></p><hr style="page-break-before:always;display:none;"><h3 class="c23 c10 c20" id="h.u2gr1kyc9imw"><span class="c31 c22"></span></h3><h3 class="c32 c10 c20" id="h.cdq9271tst5s"><span>Variant frequency data by population:</span></h3><p class="c8 c10"><span class="c9">SNP variant frequency data used in the database was obtained from the 1000 Genomes Project via Ensembl. Variant frequencies were obtained for the Finnish, Toscani Italian and British 1000 Genomes Project populations for each T1DM SNP in the EBI GWAS dataset. Finnish data was chosen as the greatest global incidence of T1D occurs in Finland due to &nbsp;Colloidal amorphous silica (ASi) present in the Finnish environment. The Toscani Italian population was chosen due to evidence of high rates of T1D in Northern Italy due to genetic risk. The British (in England and Wales) population was chosen due to increasing variance of T1D incidence in these regions. </span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span class="c9">Ensembl REST returns a list of dictionaries containing data for each study population. This function was used to request allele frequency data for 1000 Genomes Populations by SNP using a list of SNPs extracted from the GWAS dataset.</span></p><a id="t.c3e74e400e190beafce9db3e3ed60f4ca6a9f27f"></a><a id="t.9"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c38">def</span><span class="c30">&nbsp;</span><span class="c72">variant_frequency_API</span><span class="c30">(rsID):<br> &nbsp; &nbsp;</span><span class="c38">import</span><span class="c30">&nbsp;requests, sys<br><br> &nbsp; &nbsp;server = </span><span class="c70">&quot;https://rest.ensembl.org&quot;</span><span class="c30"><br> &nbsp; &nbsp;ext = </span><span class="c70">f&quot;/variation/human/{rsID}?pops=1&quot;</span><span class="c30"><br><br> &nbsp; &nbsp;r = requests.get(server+ext, headers={ </span><span class="c70">&quot;Content-Type&quot;</span><span class="c30">&nbsp;: </span><span class="c70">&quot;application/json&quot;</span><span class="c30">})<br><br> &nbsp; &nbsp;</span><span class="c38">if</span><span class="c30">&nbsp;</span><span class="c38">not</span><span class="c30">&nbsp;r.ok:<br> &nbsp; &nbsp; &nbsp;r.raise_for_status()<br> &nbsp; &nbsp; &nbsp;sys.exit()<br><br> &nbsp; &nbsp;decoded = r.json()<br> &nbsp; &nbsp;</span><span class="c38">return</span><span class="c30 c22">&nbsp;decoded</span></p></td></tr></table><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span class="c9">Variant frequency data for chosen populations were retrieved and used to create a TSV file containing the alternate allele and its frequency for all three populations for each SNP in the database. Due to some SNPs in the GWAS dataset not having an rsID, allele frequency data was excluded for SNPs at positions chr2:136066296, chr6:27349237, chr6:28594470, chr6:32978829 and chr19:42174477.</span></p><hr style="page-break-before:always;display:none;"><h3 class="c23 c10 c20" id="h.nlwo549zcwd7"><span class="c22 c31"></span></h3><h3 class="c32 c10 c20" id="h.o8avd7mg5nfg"><span class="c31 c22">Functional information and Gene Ontology:</span></h3><p class="c8 c10"><span class="c9">There are several different measures of the functional impact of a SNP CADD (Combined Annotation Dependent Depletion), a tool used for predicting the potential harm caused by genetic variants or its deleteriousness, was chosen. A CADD score indicates the likelihood of a variant being deleterious.</span></p><p class="c36 c10"><span>One advantage of CADD over other measures of functional impact such as SIFT or PolyPhen is that CADD integrates a larger and more diverse set of functional annotations. It also considers the effects of variants on non-coding regions of the genome, which can be important for understanding the functional consequences of variants that are not in protein-coding regions.</span></p><h4 class="c10 c20 c56" id="h.x1xaobp5m3xd"><span class="c14">Collecting data</span></h4><p class="c36 c10"><span>The Ensembl&#39;s Variant Effect Predictor (VEP) web tool was used to gather the Functional and Ontology data by submitting a job with the rsIDs from the T1D_GWAS_add.tsv file separated by commas.</span></p><p class="c36 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 264.00px;"><img alt="" src="images/image11.png" style="width: 601.70px; height: 264.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10 c36"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 85.33px;"><img alt="" src="images/image14.png" style="width: 601.70px; height: 85.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c36 c10"><span class="c9">From the Additional identifiers tab the following options were selected: Gene Symbol, Protein and the Gene Ontology. This allowed for us to associate the ontology terms with Genes and Protein names.</span></p><p class="c46 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 524.50px; height: 293.62px;"><img alt="" src="images/image15.png" style="width: 524.50px; height: 293.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10 c46"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 523.15px; height: 186.91px;"><img alt="" src="images/image7.png" style="width: 523.15px; height: 186.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c36 c10"><span class="c9">CADD was selected in the Prediction column to add a column containing the Raw CADD score and the CADD Phred score which then both are associated with the rsID and the Genes and Protein names.</span></p><p class="c46 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 502.50px; height: 282.97px;"><img alt="" src="images/image13.png" style="width: 502.50px; height: 282.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10"><span>Figure 1: Screenshot of the Job submitting Ensembl webpage (</span><span class="c61"><a class="c28" href="https://www.google.com/url?q=https://www.ensembl.org/Homo_sapiens/Tools/VEP&amp;sa=D&amp;source=editors&amp;ust=1677611574228054&amp;usg=AOvVaw1SLg3oXCtWD3RerKvDgaO2">https://www.ensembl.org/Homo_sapiens/Tools/VEP</a></span><span class="c9">)</span></p><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 181.33px;"><img alt="" src="images/image6.png" style="width: 601.70px; height: 181.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c36 c10"><span class="c9">The VEP file provides detailed information about the functional and ontological consequences of genetic variants, including their impact on genes, proteins, and pathways.</span></p><h4 class="c56 c10 c20" id="h.k6rssmwl5vmz"><span class="c14">Trimming data</span></h4><p class="c8 c10"><span class="c9">After running the job, we get an output text file with several columns however we are only interested in the following:</span></p><p class="c8 c10"><span class="c25">Uploaded_Variation:</span><span class="c9">&nbsp;the reference SNP identifier for the variant.</span></p><p class="c8 c10"><span class="c25">Allele:</span><span class="c9">&nbsp;the alternative allele observed at the variant site.</span></p><p class="c8 c10"><span class="c25">location:</span><span class="c9">&nbsp;the location of the variant within the affected gene</span></p><p class="c8 c10"><span class="c25">Gene: </span><span class="c9">the Ensembl gene ID of the affected gene.</span></p><p class="c8 c10"><span class="c25">Symbol:</span><span class="c9">&nbsp;the gene symbol or name.</span></p><p class="c8 c10"><span class="c25">CADD_PHRED:</span><span class="c9">&nbsp;Phred-scaled CADD score (Combined Annotation-Dependent Depletion), which predicts the deleteriousness of variants.</span></p><p class="c8 c10"><span class="c25">CADD_RAW:</span><span class="c9">&nbsp;the raw CADD score, which is a measure of the deleteriousness of variants.</span></p><p class="c8 c10"><span class="c25">GO Terms:</span><span class="c9">&nbsp;Gene Ontology (GO) terms associated with the affected gene.</span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span class="c9">The Functional information was extracted using the following script utilising pandas dataframe function to select the required columns and then converted back in to a new csv file using pandas &ldquo;to_csv()&rdquo; function.</span></p><a id="t.f3f9fb74279c969d12c7a6c83b2d796a6a37de06"></a><a id="t.10"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c27">import pandas </span><span class="c35">as</span><span class="c27">&nbsp;pd<br>Association_table_filename = </span><span class="c17">&#39;Functional_and_Ontology_data.tsv&#39;</span><span class="c27"><br>df = pd.read_csv(Association_table_filename, sep=</span><span class="c17">&#39;\t&#39;</span><span class="c27">)<br>columns_to_keep = [</span><span class="c17">&#39;Uploaded_variation&#39;</span><span class="c27">,</span><span class="c17">&#39;Allele&#39;</span><span class="c27">,</span><span class="c17">&#39;CADD_PHRED&#39;</span><span class="c27">, </span><span class="c17">&#39;CADD_RAW&#39;</span><span class="c27">, </span><span class="c17">&#39;PolyPhen&#39;</span><span class="c27">, </span><span class="c17">&#39;SIFT&#39;</span><span class="c27">]<br>df = df[columns_to_keep]<br>print(df)<br><br>df.to_csv(</span><span class="c17">&quot;Func_trimmed.csv&quot;</span><span class="c27">, index=</span><span class="c35">False</span><span class="c27">)</span></p></td></tr></table><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span class="c9">The Gene ontology data was filtered in a similar way using the following script.</span></p><a id="t.27821d57d0958ef65426cc9e6c2b2f4c73afe9da"></a><a id="t.11"></a><table class="c37"><tr class="c16"><td class="c52" colspan="1" rowspan="1"><p class="c8"><span class="c19">import</span><span class="c18">&nbsp;pandas </span><span class="c19">as</span><span class="c18">&nbsp;pd<br>Association_table_filename = </span><span class="c15">&#39;Functional_and_Ontology_data.tsv&#39;</span><span class="c18"><br>df = pd.read_csv(Association_table_filename, sep=</span><span class="c15">&#39;\t&#39;</span><span class="c18">)<br>columns_to_keep = [</span><span class="c15">&#39;Uploaded_variation&#39;</span><span class="c18">,</span><span class="c15">&#39;Location&#39;</span><span class="c18">,</span><span class="c15">&#39;SYMBOL&#39;</span><span class="c18">, </span><span class="c15">&#39;Gene&#39;</span><span class="c18">, </span><span class="c15">&#39;GO&#39;</span><span class="c18">]<br>df = df[columns_to_keep]<br>print(df)<br><br>df.to_csv(</span><span class="c15">&quot;GO_trimmed.csv&quot;</span><span class="c18">, index=</span><span class="c19">False</span><span class="c18">)</span></p></td></tr></table><p class="c8 c10"><span>&nbsp;</span></p><p class="c36 c10"><span class="c9">We have converted the text file to a tsv file and trimmed down the file to include only the rsID, Alleles, CADD_PHRED and CADD_RAW scores columns for the functional data and the rsID, location, gene, symbol, GO terms columns for the Ontology data. We have further used these TSV files in our database.</span></p><h4 class="c36 c10 c20" id="h.2qyx09azowbm"><span class="c14">Outputting Functional information and Gene Ontology</span></h4><p class="c8 c10"><span class="c9">We have further coded using flask to give the functional data and the GO terms as output when searched by rsID, Gene name and Chromosome position. The following code was used for it:</span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span>TALK ABOUT DBreq() implementing code in to flask&hellip;</span><hr style="page-break-before:always;display:none;"></p><h3 class="c10 c20 c32" id="h.x88z4oadkpbf"><span class="c31 c22">Linkage Disequilibrium:</span></h3><p class="c36"><span>Linkage disequilibrium (LD) is the degree of non-random association of the allele of one SNP with the allele of another SNP within a population. LD is typically measured by two metrics: D&rsquo; and r</span><span class="c54">2</span><span class="c9">. </span></p><p class="c36"><span class="c9">D&rsquo; is the normalised values of D, the coefficient of linkage disequilibrium, where A and B are alleles of two SNPs in different loci:</span></p><p class="c46"><img src="images/image1.png"><span class="c55 c22 c73">&nbsp;</span></p><p class="c36"><img src="images/image2.png"><span class="c54">&nbsp; </span><span class="c9">is the correlation coefficient between two loci:</span></p><p class="c46"><img src="images/image3.png"></p><h4 class="c75" id="h.9g8mzey51un2"><span class="c14">Collecting data</span></h4><p class="c36"><span>Linkage disequilibrium data was obtained from LDlink using the LDmatrix tool. D&rsquo; and r</span><span class="c54">2</span><span>&nbsp;values for SNPs were calculated using 1000 Genomes Project data for all three populations. LD data was obtained by inputting a list of SNPs from the same chromosome and selecting the population which would be used for allele frequency data for LD calculations. LDmatrix would produce two text files containing a matrix of results for D&rsquo; and r</span><span class="c54">2</span><span class="c9">&nbsp;values calculated between all SNPs pair combinations in the input list. This was performed separately for each population. Some SNPs did not have any LD data due to a lack of allele frequency data for those SNPs in the 1000 Genomes Project.</span></p><p class="c8"><span>LD datasets containing D&rsquo; and r</span><span class="c54">2</span><span class="c9">&nbsp;values for Finnish, Toscani and British populations are loaded in with pandas as separate dataframes. Each dataframe has their index set to the first column which contains SNP rsIDs.</span></p><a id="t.2d68cf246bdeee4863dcdfbc42860528c30c8e2e"></a><a id="t.12"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c44">import</span><span class="c21">&nbsp;pandas </span><span class="c44">as</span><span class="c21">&nbsp;pd<br></span><span class="c53"># Finland (FIN)</span><span class="c21"><br>LD_D_FIN = pd.read_table(</span><span class="c6">&#39;FIN_D.txt&#39;</span><span class="c21">)<br>LD_D_FIN = LD_D_FIN.set_index(</span><span class="c6">&#39;RS_number&#39;</span><span class="c21">)<br>LD_r2_FIN = pd.read_table(</span><span class="c6">&#39;FIN_r2.txt&#39;</span><span class="c21">)<br>LD_r2_FIN = LD_r2_FIN.set_index(</span><span class="c6">&#39;RS_number&#39;</span><span class="c21">)<br></span><span class="c53"># Italy - Tuscany (TSI)</span><span class="c21"><br>LD_D_TSI = pd.read_table(</span><span class="c6">&#39;TSI_D.txt&#39;</span><span class="c21">)<br>LD_D_TSI = LD_D_TSI.set_index(</span><span class="c6">&#39;RS_number&#39;</span><span class="c21">)<br>LD_r2_TSI = pd.read_table(</span><span class="c6">&#39;TSI_r2.txt&#39;</span><span class="c21">)<br>LD_r2_TSI = LD_r2_TSI.set_index(</span><span class="c6">&#39;RS_number&#39;</span><span class="c21">)<br></span><span class="c53"># British (GBR)</span><span class="c21"><br>LD_D_GBR = pd.read_table(</span><span class="c6">&#39;GBR_D.txt&#39;</span><span class="c21">)<br>LD_D_GBR = LD_D_GBR.set_index(</span><span class="c6">&#39;RS_number&#39;</span><span class="c21">)<br>LD_r2_GBR = pd.read_table(</span><span class="c6">&#39;GBR_r2.txt&#39;</span><span class="c21">)<br>LD_r2_GBR = LD_r2_GBR.set_index(</span><span class="c6">&#39;RS_number&#39;</span><span class="c21">)</span></p></td></tr></table><p class="c8 c11"><span class="c55 c22 c33"></span></p><p class="c8"><span class="c9">This function uses the itertools combination function to create a list of tuples containing all unique pairs of SNPs possible from a list of SNPs. The list is then separated into two lists containing the first and second element of each tuple.</span></p><a id="t.3cd888f33c1969e34d6e88ae1b5769c14c614333"></a><a id="t.13"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c24"># Take list of SNPs and creates a pair of lists containing the 1st and 2nd SNP of each combination &nbsp;</span><span class="c5"><br></span><span class="c2">def</span><span class="c5">&nbsp;</span><span class="c69">SNP_pair_lists</span><span class="c5">(SNP_list):<br> &nbsp; &nbsp;SNP_combinations = list(itertools.combinations(SNP_list,</span><span class="c12">2</span><span class="c5">))<br> &nbsp; &nbsp;SNP_1_list = []<br> &nbsp; &nbsp;SNP_2_list = []<br> &nbsp; &nbsp;</span><span class="c2">for</span><span class="c5">&nbsp;SNP_pair </span><span class="c2">in</span><span class="c5">&nbsp;SNP_combinations:<br> &nbsp; &nbsp; &nbsp; &nbsp;SNP_1_list.append(SNP_pair[</span><span class="c12">0</span><span class="c5">])<br> &nbsp; &nbsp; &nbsp; &nbsp;SNP_2_list.append(SNP_pair[</span><span class="c12">1</span><span class="c5">])<br> &nbsp; &nbsp;</span><span class="c2">return</span><span class="c5">&nbsp;SNP_1_list, SNP_2_list<br><br>SNP_1_list, SNP_2_list = SNP_pair_lists(SNP_list)</span></p></td></tr></table><p class="c8 c11"><span class="c55 c22 c33"></span></p><p class="c8"><span class="c9">An empty dataframe is created to be filled with rows containing data from all six dataframes. This loop uses the two lists created from the SNP list to index each dataframe and extract the respective LD value. These are used to create a list which is converted into a single row pandas dataframe which is added to the empty dataframe using pandas concat until data for all relevant pairwise LD calculations have been added. The completed dataframe is then outputted as a TSV file.</span></p><a id="t.ab61ca324e606d6f89116ed6d22747e2ed98beb9"></a><a id="t.14"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c24"># Create Empty dataframe </span><span class="c5"><br>LD_dataset = pd.DataFrame(columns=[</span><span class="c45">&#39;SNP_1&#39;</span><span class="c5">, </span><span class="c45">&#39;SNP_2&#39;</span><span class="c5">, </span><span class="c45">&#39;FIN_D\&#39;&#39;</span><span class="c5">, </span><span class="c45">&#39;FIN_r2&#39;</span><span class="c5">, </span><span class="c45">&#39;TSI_D\&#39;&#39;</span><span class="c5">, </span><span class="c45">&#39;TSI_r2&#39;</span><span class="c5">, </span><span class="c45">&#39;GBR_D\&#39;&#39;</span><span class="c5">, </span><span class="c45">&#39;GBR_r2&#39;</span><span class="c5">])<br></span><span class="c24"># Indexes the respective LD calculation for each pair and adds it to the data</span><span class="c5"><br></span><span class="c2">for</span><span class="c5">&nbsp;SNP_1,SNP_2 </span><span class="c2">in</span><span class="c5">&nbsp;zip(SNP_1_list,SNP_2_list):<br> &nbsp; &nbsp;</span><span class="c24"># Finland</span><span class="c5"><br> &nbsp; &nbsp;FIN_D = LD_D_FIN[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;FIN_r2 = LD_r2_FIN[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;</span><span class="c24"># Italy - Tuscany</span><span class="c5"><br> &nbsp; &nbsp;TSI_D = LD_D_TSI[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;TSI_r2 = LD_r2_TSI[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;</span><span class="c24"># British</span><span class="c5"><br> &nbsp; &nbsp;GBR_D = LD_D_GBR[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;GBR_r2 = LD_r2_GBR[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;</span><span class="c24"># Create row of data and combine with LD dataset dataframe</span><span class="c5 c22"><br> &nbsp; &nbsp;row_list = [SNP_1, SNP_2, FIN_D, FIN_r2, TSI_D, TSI_r2, GBR_D, GBR_r2]<br> &nbsp; &nbsp;row = pd.DataFrame(row_list).T</span></p><p class="c8"><span class="c5"><br> &nbsp; &nbsp;row.columns = LD_dataset.columns<br> &nbsp; &nbsp;LD_dataset = pd.concat([LD_dataset, row])<br></span><span class="c24"># Write out LD dataset as a TSV</span><span class="c5"><br>LD_dataset.to_csv(</span><span class="c45">&#39;LD_T1DM_Chr6.tsv&#39;</span><span class="c5">, sep=</span><span class="c45">&quot;\t&quot;</span><span class="c5">, index=</span><span class="c2">False</span><span class="c5">)</span></p></td></tr></table><p class="c8 c11"><span class="c55 c22 c33"></span></p><p class="c8 c11"><span class="c55 c22 c33"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c23 c20" id="h.czvflz3vh9cw"><span class="c22 c59"></span></h3><h4 class="c65 c10 c20" id="h.2wm5ip20i7c1"><span>Outputting LD results:</span></h4><p class="c8"><span class="c9">When a user searches by gene name or chromosomal coordinates, if multiple SNPs are returned, a list of SNPs is used to filter the LD dataset for all rows with entries for all pairwise LD calculations of SNPs in the list and output a results dataframe.</span></p><p class="c8 c11"><span class="c9"></span></p><p class="c8"><span class="c9">Before filtering, the list is checked for any SNPs which are not in the LD dataset due to lack of LD data and any offending SNPs are removed from the list. </span></p><a id="t.2485981c42700898c52a2bdcff2edf8fc12f566c"></a><a id="t.15"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c2">def</span><span class="c5">&nbsp;</span><span class="c69">remove_invalid_SNPs</span><span class="c5">(SNP_list, LD_dataset_file = </span><span class="c45">&quot;data/TSVs/LD_T1DM_Chr6.tsv&quot;</span><span class="c5">):<br></span><span class="c24"># remove SNPs returned from query which have no LD values in LD dataset</span><span class="c5"><br> &nbsp; &nbsp;</span><span class="c24"># Load LD dataset as pandas dataframe</span><span class="c5"><br> &nbsp; &nbsp;LD_df = pd.read_table(LD_dataset_file)<br> &nbsp; &nbsp;</span><span class="c24"># checks for SNPs in subset which are not in LD dataset</span><span class="c5"><br> &nbsp; &nbsp;invalid_list = []<br> &nbsp; &nbsp;</span><span class="c2">for</span><span class="c5">&nbsp;SNP </span><span class="c2">in</span><span class="c5">&nbsp;SNP_list:<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c2">if</span><span class="c5">&nbsp;SNP </span><span class="c2">not</span><span class="c5">&nbsp;</span><span class="c2">in</span><span class="c5">&nbsp;LD_df[</span><span class="c45">&#39;SNP_1&#39;</span><span class="c5">].tolist(): </span><span class="c24"># check if SNP is in LD dataset</span><span class="c5"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;invalid_list.append(SNP) </span><span class="c24"># add to list of invalid SNPs</span><span class="c5"><br> &nbsp; &nbsp;print(invalid_list)<br> &nbsp; &nbsp;</span><span class="c24"># remove invalid SNPs from SNP list passed to LD plot</span><span class="c5"><br> &nbsp; &nbsp;</span><span class="c2">for</span><span class="c5">&nbsp;SNP </span><span class="c2">in</span><span class="c5">&nbsp;invalid_list:<br> &nbsp; &nbsp; &nbsp; &nbsp;SNP_list.remove(SNP)<br> &nbsp; &nbsp;</span><span class="c2">return</span><span class="c5 c22">&nbsp;SNP_list<br><br>SNP_list = remove_invalid_SNPs(SNP_list)</span></p></td></tr></table><p class="c8 c11"><span class="c9"></span></p><p class="c8"><span class="c9">The SNP list is then used to create two lists containing the first and second element of each tuple using the SNP_pair_lists() function defined earlier.</span></p><a id="t.467397c9bfe0b58ffeebec61aad4b21b644362e1"></a><a id="t.16"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c24"># create a pair of lists containing the 1st and 2nd SNP of each combination</span><span class="c5"><br>SNP_1_list, SNP_2_list = SNP_pair_lists(SNP_list)</span></p></td></tr></table><p class="c8 c11"><span class="c9"></span></p><p class="c8"><span class="c9">The LD dataset containing all available data for pairwise LD calculations is loaded in with pandas and an empty dataframe is created for the filtered data. The pair of SNP lists are then used to index the LD dataset dataframe for all rows with pairs of SNPs relevant to the user&rsquo;s search query which are added to the LD results dataframe using pandas concat.</span></p><a id="t.258f382ba68c7d1d3a17e1bbd7a0ff9175b6a7fa"></a><a id="t.17"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c24"># Load LD dataset and create empty dataframe for filtered results</span><span class="c5"><br>LD_df = pd.read_table(LD_dataset_file)<br>LD_results_df = pd.DataFrame(columns=[</span><span class="c45">&#39;SNP_1&#39;</span><span class="c5">, </span><span class="c45">&#39;SNP_2&#39;</span><span class="c5">, </span><span class="c45">&#39;FIN_D\&#39;&#39;</span><span class="c5">, </span><span class="c45">&#39;FIN_r2&#39;</span><span class="c5">, </span><span class="c45">&#39;TSI_D\&#39;&#39;</span><span class="c5">, </span><span class="c45">&#39;TSI_r2&#39;</span><span class="c5">, </span><span class="c45">&#39;GBR_D\&#39;&#39;</span><span class="c5">, </span><span class="c45">&#39;GBR_r2&#39;</span><span class="c5">]) <br></span><span class="c24"># Loop indexing LD dataset using each pair of SNPs</span><span class="c5"><br></span><span class="c2">for</span><span class="c5">&nbsp;SNP_1,SNP_2 </span><span class="c2">in</span><span class="c5">&nbsp;zip(SNP_1_list,SNP_2_list):<br> &nbsp; &nbsp;LD_row = LD_df.loc[((LD_df[</span><span class="c45">&#39;SNP_1&#39;</span><span class="c5">] == SNP_1) &amp; (LD_df[</span><span class="c45">&#39;SNP_2&#39;</span><span class="c5">] == SNP_2) | <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(LD_df[</span><span class="c45">&#39;SNP_1&#39;</span><span class="c5">] == SNP_2) &amp; (LD_df[</span><span class="c45">&#39;SNP_2&#39;</span><span class="c5">] == SNP_1))] <br> &nbsp; &nbsp;LD_results_df = pd.concat([LD_results_df, LD_row])</span></p></td></tr></table><p class="c8 c11"><span class="c22 c33 c55"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c23 c20" id="h.tm8mndqz55x9"><span class="c59 c22"></span></h3><h4 class="c20 c65" id="h.hebu2brlzmob"><span class="c14">LD heatmap plots:</span></h4><p class="c36 c10"><span>When a user searches by gene name or chromosomal coordinates, if multiple SNPs are returned, a list of SNPs is also used to extract LD values for all relevant pairwise SNP calculations to create a dataframe used to create a heatmap plot of LD values.</span></p><p class="c8"><span class="c9">The LD dataset is loaded in with pandas and SNPs not present in the dataset are removed from the list of SNPs passed from the user query. The SNP list is then used to create two lists containing the first and second element of each tuple using the SNP_pair_lists() function defined earlier.</span></p><a id="t.37287ca283631a102efe72ff61cf4610e2d4e853"></a><a id="t.18"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c24"># load LD dataset</span><span class="c5"><br>LD_df = pd.read_table(</span><span class="c45">&#39;LD_T1DM_Chr6.tsv&#39;</span><span class="c5">)<br></span><span class="c24"># checks for SNPs in subset which are not in LD dataset</span><span class="c5 c22"><br>SNP_list = remove_invalid_SNPs(SNP_list)</span></p><p class="c8"><span class="c24"># create a pair of lists containing the 1st and 2nd SNP of each combination</span><span class="c5 c22"><br>SNP_1_list, SNP_2_list = SNP_pair_lists(SNP_list)</span></p></td></tr></table><p class="c8 c11"><span class="c55 c22 c33"></span></p><p class="c8"><span class="c9">An empty dataframe is created to be filled with LD values used to create a matrix of values used for the LD plot. The pair of SNP lists are used to index the LD dataset dataframe and extract the LD value for all possible pairwise LD calculations from the SNP list. A row of LD values is created for each SNP where each column corresponds with the pairwise LD calculation with one of the SNPs from the list. Each row is added to the empty dataframe using pandas concat.</span></p><a id="t.f562ae5cedba9ec0d08c9ebb9c8653d95d6fa722"></a><a id="t.19"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c5">LD_matrix_df = pd.DataFrame(columns=[SNP_list]) </span><span class="c24"># One column per SNP in list (Since a list object is passed, could just pass the SNP list</span><span class="c5"><br></span><span class="c2">for</span><span class="c5">&nbsp;SNP_1 </span><span class="c2">in</span><span class="c5">&nbsp;SNP_list:<br> &nbsp; &nbsp;</span><span class="c24"># Create empty list</span><span class="c5"><br> &nbsp; &nbsp;LD_value_list = []<br> &nbsp; &nbsp;</span><span class="c24"># Sub-loop - Loops to create list of datapoints</span><span class="c5"><br> &nbsp; &nbsp;</span><span class="c2">for</span><span class="c5">&nbsp;SNP_2 </span><span class="c2">in</span><span class="c5">&nbsp;SNP_list:<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c2">if</span><span class="c5">&nbsp;SNP_1 == SNP_2:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SNP_Datapoint = </span><span class="c12">1</span><span class="c5"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LD_value_list.append(SNP_Datapoint)<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c2">else</span><span class="c5">:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24">#try:</span><span class="c5"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24"># Search for specific row containing value</span><span class="c5"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LD_row = LD_df.loc[((LD_df[</span><span class="c45">&#39;SNP_1&#39;</span><span class="c5">] == SNP_1) &amp; (LD_df[</span><span class="c45">&#39;SNP_2&#39;</span><span class="c5">] == SNP_2) | <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(LD_df[</span><span class="c45">&#39;SNP_1&#39;</span><span class="c5">] == SNP_2) &amp; (LD_df[</span><span class="c45">&#39;SNP_2&#39;</span><span class="c5">] == SNP_1))] <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24"># Extract value and add to list</span><span class="c5"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SNP_Datapoint = LD_row[</span><span class="c45">&#39;GBR_r2&#39;</span><span class="c5">].tolist()[</span><span class="c12">0</span><span class="c5">] </span><span class="c24"># currently using Finnish data</span><span class="c5"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LD_value_list.append(SNP_Datapoint)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24">#except:</span><span class="c5"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24">#invalid_list.append((SNP_main,SNP_second))</span><span class="c5"><br> &nbsp; &nbsp;</span><span class="c24"># Convert into dataframe row and transpose</span><span class="c5"><br> &nbsp; &nbsp;row = pd.DataFrame(LD_value_list).T<br> &nbsp; &nbsp;row.columns = LD_matrix_df.columns<br> &nbsp; &nbsp;LD_matrix_df = pd.concat([LD_matrix_df, row])</span></p></td></tr></table><p class="c8 c11"><span class="c55 c22 c33"></span></p><hr style="page-break-before:always;display:none;"><p class="c8 c11"><span class="c9"></span></p><p class="c8"><span class="c9">The LD matrix dataframe is passed to the ld_plot function. The number of rows (n) is used to create a mask which will hide half of the heatmap to create a triangular plot. A coordinate matrix is also created to rotate the heatmap plot. The SNP list is used to create the axis labels located at the bottom of the plot. The function&rsquo;s title parameter passes a string which is used to determine the plot title.</span></p><a id="t.7434b47b22fe581bd4a9733bf8ed5a45e040d776"></a><a id="t.20"></a><table class="c37"><tr class="c16"><td class="c3" colspan="1" rowspan="1"><p class="c8"><span class="c2">def</span><span class="c5">&nbsp;</span><span class="c69">LD_plot</span><span class="c5">(LD, labels, title):<br><br> &nbsp; &nbsp;n = LD.shape[</span><span class="c12">0</span><span class="c5">]<br><br> &nbsp; &nbsp;figure = plt.figure()<br><br> &nbsp; &nbsp;</span><span class="c24"># mask triangle matrix</span><span class="c5"><br> &nbsp; &nbsp;mask = np.tri(n, k=</span><span class="c12">0</span><span class="c5">)<br> &nbsp; &nbsp;LD_masked = np.ma.array(LD, mask=mask)<br><br> &nbsp; &nbsp;</span><span class="c24"># create rotation/scaling matrix</span><span class="c5"><br> &nbsp; &nbsp;t = np.array([[</span><span class="c12">1</span><span class="c5">, </span><span class="c12">0.5</span><span class="c5">], [</span><span class="c12">-1</span><span class="c5">, </span><span class="c12">0.5</span><span class="c5">]])<br> &nbsp; &nbsp;</span><span class="c24"># create coordinate matrix and transform it</span><span class="c5"><br> &nbsp; &nbsp;coordinate_matrix = np.dot(np.array([(i[</span><span class="c12">1</span><span class="c5">], i[</span><span class="c12">0</span><span class="c5">])<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c2">for</span><span class="c5">&nbsp;i </span><span class="c2">in</span><span class="c5">&nbsp;itertools.product(range(n, </span><span class="c12">-1</span><span class="c5">, </span><span class="c12">-1</span><span class="c5">), range(</span><span class="c12">0</span><span class="c5">, n + </span><span class="c12">1</span><span class="c5">, </span><span class="c12">1</span><span class="c5">))]), t)<br> &nbsp; &nbsp;</span><span class="c24"># plot</span><span class="c5"><br> &nbsp; &nbsp;ax = figure.add_subplot(</span><span class="c12">1</span><span class="c5">, </span><span class="c12">1</span><span class="c5">, </span><span class="c12">1</span><span class="c5">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c45">&#39;bottom&#39;</span><span class="c5">].set_position(</span><span class="c45">&#39;center&#39;</span><span class="c5">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c45">&#39;top&#39;</span><span class="c5">].set_visible(</span><span class="c2">False</span><span class="c5">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c45">&#39;right&#39;</span><span class="c5">].set_visible(</span><span class="c2">False</span><span class="c5">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c45">&#39;left&#39;</span><span class="c5">].set_visible(</span><span class="c2">False</span><span class="c5">)<br> &nbsp; &nbsp;ax.get_yaxis().set_visible(</span><span class="c2">False</span><span class="c5">)<br> &nbsp; &nbsp;plt.tick_params(axis=</span><span class="c45">&#39;x&#39;</span><span class="c5">, which=</span><span class="c45">&#39;both&#39;</span><span class="c5">, top=</span><span class="c2">False</span><span class="c5">)<br> &nbsp; &nbsp;plt.pcolor(coordinate_matrix[:, </span><span class="c12">1</span><span class="c5">].reshape(n + </span><span class="c12">1</span><span class="c5">, n + </span><span class="c12">1</span><span class="c5">),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; coordinate_matrix[:, </span><span class="c12">0</span><span class="c5">].reshape(n + </span><span class="c12">1</span><span class="c5">, n + </span><span class="c12">1</span><span class="c5">), np.flipud(LD_masked), edgecolors = </span><span class="c45">&quot;white&quot;</span><span class="c5">, linewidth = </span><span class="c12">1.5</span><span class="c5">, cmap = </span><span class="c45">&#39;OrRd&#39;</span><span class="c5">)<br> &nbsp; &nbsp;plt.xticks(ticks=np.arange(len(labels)) + </span><span class="c12">0.5</span><span class="c5">, labels=labels, rotation=</span><span class="c45">&#39;vertical&#39;</span><span class="c5">, fontsize=</span><span class="c12">8</span><span class="c5">)<br> &nbsp; &nbsp;plt.colorbar()<br><br> &nbsp; &nbsp;</span><span class="c24"># add title</span><span class="c5"><br> &nbsp; &nbsp;plt.title(</span><span class="c45">f&quot;{title}&quot;</span><span class="c5">, loc = </span><span class="c45">&quot;center&quot;</span><span class="c5">)<br> &nbsp; <br> &nbsp; &nbsp;</span><span class="c2">return</span><span class="c5">&nbsp;figure<br><br>LD_heatmap_plot = ld_plot(LD_matrix_df, SNP_list, </span><span class="c45">&quot;LD plot title&quot;</span><span class="c5 c22">)</span></p></td></tr></table><p class="c8 c11"><span class="c55 c22 c33"></span></p><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 450.67px;"><img alt="" src="images/image12.png" style="width: 601.70px; height: 450.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><h3 class="c23 c10 c20" id="h.hrs2dm8dc6c5"><span class="c31 c22"></span></h3><h3 class="c23 c10 c20" id="h.j6rrdgee2fb8"><span class="c31 c22"></span></h3><h3 class="c10 c20 c23" id="h.2tuwd8vwq0u"><span class="c31 c22"></span></h3><hr style="page-break-before:always;display:none;"><h3 class="c23 c10 c20" id="h.irguhipsagxi"><span class="c31 c22"></span></h3><h3 class="c32 c10 c20" id="h.5zfnv0ig9q0m"><span class="c31 c22">Manhattan Plot:</span></h3><p class="c8 c10"><span class="c9">Manhattan plot is a type of scatter graph which displays P values of the entire Genome-wide association study (GWAS) on genomic scale. The P-values are shown in genomic order by chromosomal position on the x-axis. The y-axis shows the -log10 value of the P-value for each polymorphism. </span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span class="c9">A TSV file containing all SNPs in type 1 diabetes was produced. The Manhattan plot needs the P-value of a SNP and the chromosomal position. Using pandas, the -logp and cumulative positions were generated.</span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 292.00px;"><img alt="" src="images/image5.png" style="width: 601.70px; height: 292.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10"><span class="c9">Figure 1: Shows the original file &lsquo;GWAS_T1D.tsv&rsquo; being used to create an additional column for -logp values and creating a column for cumulative positions by using the chromosome and chromosomal position values. The cumulative position is needed to show the position of the SNP in the entire genome, rather than its position in the chromosome.</span></p><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 348.00px;"><img alt="" src="images/image10.png" style="width: 601.70px; height: 348.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10"><span class="c9">Figure 2: Creating the Manhattan plot using Bokeh. Plotting cumulative position (x) against -logp (y) and separating each chromosome by colour (grey and black). </span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span class="c9">Used Bokeh tools to make the graph user friendly. Circle plots were chosen for easy visibility and selecting of each SNP position. Hover feature was added to allow the user to hover mouse over each plot, which turns it green, and shows the rs ID value next to the chromosomal position. The select feature allows users to select 1 or more plots by either clicking one plot, or holding shift and clicking plots for multiple plots, or using the box select tool to select plots in an entire section, and turns the plots purple. The zoom features allow the user to zoom into the graph and view plots that are close to each other or seem to be overlapping for more visual clarity. The reset and undo buttons allows the user to either go back to the original plot, or undo the previous action they had committed. The save tool will save the graph as a png file with the title included. The plots were made slightly transparent to make overlapping SNPs more visible.</span></p><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 340.00px;"><img alt="" src="images/image4.png" style="width: 601.70px; height: 340.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10"><span class="c9">Figure 3: All GWAS SNPs found in Type 1 Diabetes shown in a Manhattan Plot</span></p><p class="c8 c10"><span class="c9">Users can view all SNPs for T1D, and plot the SNPs that they have searched for. This feature is available when multiple SNPs are retrieved for the region searched.</span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 578.00px; height: 323.78px;"><img alt="" src="images/image9.png" style="width: 602.00px; height: 328.00px; margin-left: -17.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10"><span class="c9">Figure 4: All GWAS SNPs for Type 1 Diabetes for only Chromosome 6</span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><h3 class="c32 c10 c20" id="h.2svyklxr9h57"><span class="c31 c22">User Feedback:</span></h3><p class="c36 c10"><span class="c9">10 biologists, ages 25-60, were given the opportunity to use the software to search for either a SNP, region or gene. They were then asked to either retrieve GO data, Linkage disequilibrium plot, or the Manhattan plot.</span></p><p class="c36 c10"><span class="c9">Things that were improved after user feedback:</span></p><ul class="c71 lst-kix_btbuo2ptomm8-0 start"><li class="c36 c10 c66 li-bullet-0"><span class="c9">Colours/Themes: users found it easier to read with a darker theme; background was made darker, themes added.</span></li><li class="c36 c66 c10 li-bullet-0"><span class="c9">Font size/style: users found the font easy to read and see, including users with visual impairments.</span></li><li class="c36 c66 c10 li-bullet-0"><span class="c9">Quick links: users did not like scrolling through to find the data they were looking for; adding the quick links makes it quicker to get to the terms.</span></li><li class="c36 c66 c10 li-bullet-0"><span class="c9">Return home button: this feature was added to all pages to allow users to return to the main page.</span></li></ul><p class="c36 c10"><span class="c9">Overall users found the genome browser very user friendly and easy to use and enjoyed all the interactive features. They said they liked how easy it is to search and retrieve information, and enlightened them on the vast amount of genome data we now have access to through GWAS. </span></p><h2 class="c8 c10 c20" id="h.hwsiupgp3f8i"><span class="c4">Flask:</span></h2><p class="c8 c10 c11"><span class="c9"></span></p><h2 class="c8 c10 c20" id="h.147jjkfr6rv5"><span class="c4">Navigation:</span></h2><p class="c8 c10"><span>Screenshot how site works how to navigate&hellip;.</span></p><h2 class="c8 c10 c20" id="h.fysf9tqgmmsh"><span class="c4">Citation:</span></h2><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><h3 class="c23 c10 c20" id="h.nrbww7njcv3p"><span class="c31 c22"></span></h3><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><p class="c8 c10 c11"><span class="c9"></span></p><h1 class="c10 c20 c48" id="h.1yu3kvn20cvs"><span class="c64"></span></h1><div><p class="c10 c11 c26"><span class="c9"></span></p></div></body></html>