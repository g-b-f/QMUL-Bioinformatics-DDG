<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=lhDjYqiy3mZ0x6ROQEUoUw');ol{margin:0;padding:0}table td,table th{padding:0}.c28{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;background-color:#1e1e1e;border-left-style:solid;border-bottom-width:0pt;width:451.4pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;background-color:#333333;border-left-style:solid;border-bottom-width:0pt;width:451.4pt;border-top-color:#000000;border-bottom-style:solid}.c49{-webkit-text-decoration-skip:none;color:#134f5c;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:20pt;font-family:"Arial";font-style:normal}.c11{background-color:#333333;color:#fcc28c;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Consolas";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c36{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c8{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c30{color:#38761d;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c10{color:#cc0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c27{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left;height:14pt}.c33{color:#c9d1d9;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c7{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#d36363;font-weight:400}.c26{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#a2fca2;font-weight:400}.c1{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#ffffff;font-weight:400}.c6{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#888888;font-weight:400}.c46{padding-top:20pt;padding-bottom:6pt;line-height:1.15;text-align:left;height:20pt}.c0{background-color:#1e1e1e;font-size:10.5pt;font-family:"Consolas";color:#dcdcdc;font-weight:400}.c3{background-color:#1e1e1e;font-size:10pt;font-family:"Consolas";color:#d69d85;font-weight:400}.c21{background-color:#1e1e1e;color:#57a64a;font-weight:400;font-family:"Consolas";font-style:italic}.c24{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#fcc28c;font-weight:400}.c40{background-color:#333333;font-size:9pt;font-family:"Consolas";color:#ffffaa;font-weight:400}.c14{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c35{background-color:#1e1e1e;font-family:"Consolas";color:#d69d85;font-weight:400}.c18{background-color:#1e1e1e;font-family:"Consolas";color:#b8d7a3;font-weight:400}.c45{padding-top:24pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c13{background-color:#1e1e1e;font-family:"Consolas";color:#569cd6;font-weight:400}.c41{color:#434343;font-weight:400;font-size:14pt;font-family:"Arial"}.c44{padding-top:0pt;padding-bottom:3pt;line-height:1.15;text-align:left}.c22{border-spacing:0;border-collapse:collapse;margin-right:auto}.c42{color:#000000;font-weight:400;font-size:13pt;font-family:"Arial"}.c47{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:center}.c4{background-color:#1e1e1e;font-family:"Consolas";color:#dcdcdc;font-weight:400}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c37{padding-top:16pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c48{color:#212121;font-weight:400;font-family:"Arial"}.c43{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c39{text-decoration:none;vertical-align:baseline;font-style:normal}.c29{text-decoration:none;vertical-align:baseline;font-size:11pt}.c50{text-decoration:none;vertical-align:baseline;font-style:italic}.c12{orphans:2;widows:2}.c16{vertical-align:super}.c38{height:322.3pt}.c25{height:0pt}.c15{font-size:10.5pt}.c23{font-size:10pt}.c31{page-break-after:avoid}.c19{height:11pt}.c34{height:16pt}.c32{height:14pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:0pt;color:#38761d;font-weight:700;font-size:16pt;padding-bottom:0pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#cc0000;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c43 doc-content"><p class="c12 c31 c44 title" id="h.caspr5vic333"><span class="c5">READ ME</span></p><h2 class="c9 c12 c31" id="h.bbkfn1omw6w2"><span class="c30">What is it?:</span></h2><p class="c9 c12"><span class="c2">This web application prototype is designed to retrieve information on Single Nucleotide Polymorphisms (SNPs) seen in Type 1 Diabetes patients identified by Genome wide association studies (GWAS). The database will use information from the GWAS catalogue, along with population data from Ensembl and the 1000 Genomes Project and functional information and Gene Ontology information obtained through Ensembl&rsquo;s VEP tool which is all is retrievable through a user friendly interface through the input of an rsID, Chromosome position or a Gene name. The site also allows the user to calculate Linkage Disequilibrium (LD) of SNPs selected for each population producing a text file containing the LD values and plot these values as a LD heatmap. The user is also able to enter multiple SNPs and return a Manhattan plot of the p-values.</span></p><h2 class="c9 c12 c31" id="h.ub1lvyokxtm"><span class="c30">Prerequisites:</span></h2><h3 class="c8" id="h.q7ze64iletz1"><span class="c10">Functions used throughout:</span></h3><p class="c9 c12"><span>A number of functions were used throughout the code such as &hellip;</span></p><a id="t.6570bd816ac0e608aa8951bc3c0e591455181d70"></a><a id="t.0"></a><table class="c22"><tr class="c25"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c13">def</span><span class="c4">&nbsp;removeDupeSNP(dataframe): &nbsp; &nbsp; <br></span><span class="c21"># Removes duplicates from a pandas dataframe, leaving only greatest p-value</span><span class="c4"><br> &nbsp; dataframe.reset_index(drop=</span><span class="c13">True</span><span class="c4">) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span><span class="c21"># Resets index back to 0</span><span class="c4"><br> &nbsp; dupeList = dataframe.duplicated(subset=</span><span class="c35">&#39;snps&#39;</span><span class="c4">,keep=</span><span class="c13">False</span><span class="c4">) &nbsp; <br></span><span class="c21"># Get list of duplicate values</span><span class="c4"><br> &nbsp; dupes=dataframe[dupeList] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c21 c29"># Select dataframe using above list</span></p><p class="c9"><span class="c0">&nbsp; &nbsp;dupesDict={}<br> &nbsp; </span><span class="c13 c15">for</span><span class="c0">&nbsp;index,row </span><span class="c13 c15">in</span><span class="c0">&nbsp;dupes.iterrows(): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span><span class="c21 c15"># Iterate through df of duplicates, one row at a time</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; rsVal=row[</span><span class="c35 c15">&quot;snps&quot;</span><span class="c0">] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c21 c15"># SNP name (rs value)</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; snpTuple=(index,row[</span><span class="c35 c15">&quot;p_value&quot;</span><span class="c0">]) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c21 c15"># Tuple containing index and p-val</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; </span><span class="c13 c15">if</span><span class="c0">&nbsp;rsVal </span><span class="c13 c15">in</span><span class="c0">&nbsp;dupesDict: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span><span class="c21 c15"># If it&#39;s seen the snp before,</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dictList=dupesDict[rsVal] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c21 c15"># go to the value for the snp,</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dictList.append(snpTuple) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c21 c15"># and add the index/ p-val tuple.</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; </span><span class="c13 c15">else</span><span class="c0">: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br></span><span class="c15 c21"># If it hasn&#39;t seen the snp before,</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dupesDict.update({rsVal:[snpTuple]}) &nbsp; &nbsp;<br></span><span class="c21 c15"># create a listing for it.</span></p></td></tr></table><p class="c9 c19"><span class="c0 c39"><br><br><br></span></p><p class="c9 c19"><span class="c4"><br><br></span></p><a id="t.a725f3f15ee101df28c7684965a6bd8a25eed434"></a><a id="t.1"></a><table class="c22"><tr class="c38"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c0">&nbsp; &nbsp;naughtyList=[] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c21 c15"># List of lists of (index, p-val) we want to drop</span><span class="c0"><br> &nbsp; </span><span class="c13 c15">for</span><span class="c0">&nbsp;i </span><span class="c13 c15">in</span><span class="c0">&nbsp;dupesDict:<br> &nbsp; &nbsp; &nbsp; snp = dupesDict[i] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c21 c15"># Get list of (index, pVal)</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; sortByP=sorted(snp,key=</span><span class="c13 c15">lambda</span><span class="c0">&nbsp;x: </span><span class="c18 c15">0</span><span class="c0">-x[</span><span class="c18 c15">1</span><span class="c0">]) &nbsp; &nbsp;</span><span class="c21 c15"># Sort by p-value</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; sortByP=sortByP[</span><span class="c18 c15">1</span><span class="c0">:] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c21 c15"># Select all but greatest p value</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; naughtyList.append(sortByP)<br><br><br> &nbsp; dropList=[] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c21 c15"># List of indices for rows we want to drop</span><span class="c0"><br> &nbsp; </span><span class="c13 c15">for</span><span class="c0">&nbsp;i </span><span class="c13 c15">in</span><span class="c0">&nbsp;naughtyList: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c21 c15"># Enter first list</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; </span><span class="c13 c15">for</span><span class="c0">&nbsp;j </span><span class="c13 c15">in</span><span class="c0">&nbsp;i: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c21 c15"># Enter second list</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dropList.append(j[</span><span class="c15 c18">0</span><span class="c0">]) &nbsp; &nbsp; &nbsp; </span><span class="c21 c15"># Add the index from each tuple</span><span class="c0"><br><br><br> &nbsp; </span><span class="c13 c15">return</span><span class="c0">(dataframe.drop(dropList)) &nbsp; &nbsp;</span><span class="c21 c15"># Return dataframe without duplicate values</span></p></td></tr></table><p class="c9 c19"><span class="c0"><br><br><br></span></p><a id="t.59b2d73cce28e0f846926b398a5396f1238f472c"></a><a id="t.2"></a><table class="c22"><tr class="c25"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c13 c15">def</span><span class="c0">&nbsp;removeDupeGeneMap(GeneMap):<br> &nbsp; </span><span class="c13 c15">try</span><span class="c0">:<br> &nbsp; &nbsp; &nbsp; GeneMap=GeneMap.split(</span><span class="c35 c15">&#39;, &#39;</span><span class="c0">)<br> &nbsp; &nbsp; &nbsp; uniques=</span><span class="c35 c15">&quot;&quot;</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; </span><span class="c13 c15">for</span><span class="c0">&nbsp;item </span><span class="c13 c15">in</span><span class="c0">&nbsp;GeneMap:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13 c15">if</span><span class="c0">&nbsp;item </span><span class="c13 c15">not</span><span class="c0">&nbsp;</span><span class="c13 c15">in</span><span class="c0">&nbsp;uniques: </span><span class="c21 c15"># If the item hasn&#39;t been seen before,</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; uniques+=(item) &nbsp; &nbsp; </span><span class="c21 c15"># add it to the list.</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; uniques+=(</span><span class="c35 c15">&quot;, &quot;</span><span class="c0">) &nbsp; &nbsp; </span><span class="c21 c15"># Also add &#39; ,&#39;</span><span class="c0"><br> &nbsp; &nbsp; &nbsp; </span><span class="c13 c15">return</span><span class="c0">&nbsp;(uniques[:</span><span class="c18 c15">-2</span><span class="c0">]) &nbsp; &nbsp; &nbsp; </span><span class="c21 c15"># Remove last &#39; ,&#39;</span><span class="c0"><br> &nbsp; </span><span class="c13 c15">except</span><span class="c0">:<br> &nbsp; &nbsp; &nbsp; </span><span class="c13 c15">return</span><span class="c0">&nbsp;(</span><span class="c35 c15">&quot;Data unavailable&quot;</span><span class="c0">) &nbsp; &nbsp; </span><span class="c21 c15"># Return this if geneMap is empty</span></p></td></tr></table><h2 class="c9 c12 c31 c34" id="h.vhchvtb4vp8x"><span class="c30"></span></h2><h2 class="c9 c12 c31" id="h.exepwhhdz8ou"><span>Structure:</span></h2><p class="c9 c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 412.00px;"><img alt="" src="images/image4.png" style="width: 601.70px; height: 412.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9 c12 c19"><span class="c2"></span></p><h3 class="c8" id="h.9yfmc6qy2clu"><span class="c10">GWAS:</span></h3><p class="c9 c12"><span class="c2">This information was downloaded from the GWAS catalogue where a TSV file was downloaded and then trimmed using the following code;</span></p><a id="t.b1af7d7bc328582cfe763ff0e0cbb9d8bcbeb335"></a><a id="t.3"></a><table class="c22"><tr class="c25"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c4 c23">fileIn = getPath(</span><span class="c3">&#39;gwas_catalog_v1.0-associations_e108_r2023-01-14.tsv&#39;</span><span class="c4 c23">) </span><span class="c21 c23"># https://www.ebi.ac.uk/gwas/docs/file-downloads</span><span class="c4 c23"><br>fileOut = getPath(</span><span class="c3">&#39;gwas_trimmed.tsv&#39;</span><span class="c4 c23">)<br><br>data = pd.read_csv(fileIn, sep=</span><span class="c3">&#39;\t&#39;</span><span class="c4 c23">, low_memory=</span><span class="c13 c23">False</span><span class="c4 c23">) &nbsp; &nbsp;</span><span class="c21 c23"># Reads gwas tsv</span><span class="c4 c23"><br>data=removeSpecial(data) &nbsp; &nbsp;</span><span class="c21 c23"># removes special characters in column names</span></p></td></tr></table><p class="c9 c12"><span class="c2">This code uses pandas to open the TSV file, creates a dataframe called data, and removes any special characters from the column names, as SQL does not interact with special characters very well.</span></p><a id="t.638fb5b406f87bace7364f8ea9275d9a6c56f7f3"></a><a id="t.4"></a><table class="c22"><tr class="c25"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c4">data=data.query(</span><span class="c35">&quot;disease_trait==&#39;Type 1 diabetes&#39; or study.str.contains(&#39;type 1 diabetes&#39;)&quot;</span><span class="c4">)<br>data = data.loc[data.snps.str.contains(</span><span class="c35">r&#39;rs[0-9]+&#39;</span><span class="c4">)] &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c21"># get only snps with rsids</span><span class="c4"><br></span><span class="c21">#data = data.loc[data[&#39;CHR_ID&#39;]==&#39;6&#39;] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Select only rows for chromosome 6</span></p></td></tr></table><p class="c9 c12"><span>The data frame is then filtered further so that it only has data that references T1D in the &ldquo;disease_trait&rdquo; column so that only T1D data remains in the dataframe. Next all SNPs that don&#39;t have rsIDs are removed, as some cells had incompatible data in this column.</span></p><a id="t.132f647eb56f42c8d7dbb1fffca78301f723bdb3"></a><a id="t.5"></a><table class="c22"><tr class="c25"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c4 c23">data = data[[</span><span class="c3">&quot;snps&quot;</span><span class="c4 c23">,</span><span class="c3">&quot;region&quot;</span><span class="c4 c23">,</span><span class="c3">&quot;chr_pos&quot;</span><span class="c4 c23">,</span><span class="c3">&quot;chr_id&quot;</span><span class="c4 c23">,</span><span class="c3">&quot;p_value&quot;</span><span class="c4 c23">,</span><span class="c3">&quot;mapped_gene&quot;</span><span class="c4 c23">]]</span></p></td></tr></table><p class="c9 c12"><span>The da</span><span class="c2">taframe is then trimmed again so that it contains only the columns of interest &nbsp; &quot;snps&quot;, &quot;region&quot;, &quot;chr_pos&quot;, &quot;chr_id&quot;, &quot;p_value&quot;, &quot;mapped_gene&quot;. </span></p><p class="c9 c12 c19"><span class="c2"></span></p><a id="t.8adbe8fae2f67de92759b4b70695249a62c02e6d"></a><a id="t.6"></a><table class="c22"><tr class="c25"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c4">data=removeDupeSNP(data) &nbsp; &nbsp;</span><span class="c21"># Remove duplicates (leaving the entry with largest p value)</span><span class="c4"><br>newCol=[removeDupeGeneMap(r[</span><span class="c35">&quot;mapped_gene&quot;</span><span class="c4">]) </span><span class="c13">for</span><span class="c4">&nbsp;i, r </span><span class="c13">in</span><span class="c4">&nbsp;data.iterrows()]</span></p><p class="c9"><span class="c4">data[</span><span class="c35">&quot;mapped_gene&quot;</span><span class="c4">]=newCol</span></p></td></tr></table><p class="c9 c12"><span class="c2">Duplicate mapped_gene information is then removed.</span></p><p class="c9 c12 c19"><span class="c2"></span></p><a id="t.efbc0c483d3f71e27abdb0a27605856edba6693c"></a><a id="t.7"></a><table class="c22"><tr class="c25"><td class="c28" colspan="1" rowspan="1"><p class="c9"><span class="c4 c23">data.rename(columns = {</span><span class="c3">&#39;snps&#39;</span><span class="c4 c23">:</span><span class="c3">&#39;rsid&#39;</span><span class="c4 c23">}, inplace = </span><span class="c13 c23">True</span><span class="c4 c23">)<br><br></span><span class="c21 c23"># if os.path.exists(fileOut): # If the file exists,</span><span class="c4 c23"><br></span><span class="c21 c23"># &nbsp; &nbsp; os.remove(fileOut) &nbsp; &nbsp; # delete it.</span><span class="c4 c23"><br>data.to_csv(fileOut, sep=</span><span class="c3">&#39;\t&#39;</span><span class="c4 c23">, index=</span><span class="c13 c23">False</span><span class="c4 c23">)</span></p></td></tr></table><p class="c9 c12"><span>Next the column &#39;snps&#39; was renamed to &#39;rsid&#39; and made the change directly to the dataframe by setting inplace=True.</span></p><h3 class="c8" id="h.o8avd7mg5nfg"><span class="c10">Functional information and Gene Ontology:</span></h3><p class="c14 c12"><span class="c2">We have used Ensembl&#39;s Variant Effect Predictor web tool to gather the Functional and Ontology data by submitting a job with the rsIDs as input from the above GWAS TSV file. After running the job, we get an output of text file with columns like:</span></p><p class="c14 c12"><span class="c2">rsid: the reference SNP identifier for the variant.</span></p><p class="c14 c12"><span class="c2">Allele: the alternative allele observed at the variant site.</span></p><p class="c14 c12"><span class="c2">impact: the impact of the variant on the affected gene</span></p><p class="c14 c12"><span class="c2">consequence: the consequence of the variant on the affected gene</span></p><p class="c14 c12"><span class="c2">location: the location of the variant within the affected gene</span></p><p class="c14 c12"><span class="c2">Gene: the Ensembl gene ID of the affected gene.</span></p><p class="c14 c12"><span class="c2">Symbol: the gene symbol or name.</span></p><p class="c14 c12"><span class="c2">Feature_Type: the type of genomic feature the variant is located in</span></p><p class="c14 c12"><span class="c2">Feature: the Ensembl ID of the specific feature the variant is located in</span></p><p class="c14 c12"><span class="c2">Exon: the exon number(s) affected by the variant.</span></p><p class="c14 c12"><span class="c2">Intron: the intron number(s) affected by the variant.</span></p><p class="c14 c12"><span class="c2">HGVSc: the HGVS nomenclature for the variant at the cDNA .</span></p><p class="c14 c12"><span class="c2">HGVSp: the HGVS nomenclature for the variant at the protein level.</span></p><p class="c14 c12"><span class="c2">cDNA_position: the position of the variant within the cDNA sequence of the affected gene.</span></p><p class="c14 c12"><span class="c2">CDS_position: the position of the variant within the coding sequence of the affected gene.</span></p><p class="c14 c12"><span class="c2">Protein_position: the position of the variant within the protein sequence of the affected gene.</span></p><p class="c14 c12"><span class="c2">Amino_acids: the amino acid change resulting from the variant.</span></p><p class="c14 c12"><span class="c2">Codons: the DNA codon change resulting from the variant.</span></p><p class="c14 c12"><span class="c2">Existing_variation: additional identifiers for the variant in other databases.</span></p><p class="c14 c12"><span class="c2">Distance: the distance to the nearest feature in the same or opposite strand.</span></p><p class="c14 c12"><span class="c2">Strand: the genomic strand the variant is located on.</span></p><p class="c14 c12"><span class="c2">FLAGS: additional information about the variant .</span></p><p class="c14 c12"><span class="c2">SYMBOL_SOURCE: the source of the gene symbol or name.</span></p><p class="c14 c12"><span class="c2">HGNC_ID: the HGNC gene ID of the affected gene.</span></p><p class="c14 c12"><span class="c2">MANE_SELECT: indication of whether this transcript is the MANE (Matched Annotation from NCBI and EMBL-EBI) Select transcript.</span></p><p class="c14 c12"><span class="c2">MANE_PLUS_CLINICAL: indication of whether this transcript is the MANE Select Plus Clinical (MPC) transcript.</span></p><p class="c14 c12"><span class="c2">TSL: transcript support level (a measure of transcript annotation confidence).</span></p><p class="c14 c12"><span class="c2">APPRIS: annotation of principal isoforms for each gene.</span></p><p class="c14 c12"><span class="c2">ENSP: the Ensembl protein ID of the affected protein.</span></p><p class="c14 c12"><span class="c2">SIFT: prediction of the effect of the variant on protein function.</span></p><p class="c14 c12"><span class="c2">PolyPhen: prediction of the effect of the variant on protein function.</span></p><p class="c14 c12"><span class="c2">CLIN_SIG: clinical significance of the variant.</span></p><p class="c14 c12"><span class="c2">SOMATIC: indication of whether the variant is somatic or germline.</span></p><p class="c14 c12"><span class="c2">PHENO: phenotype association of the variant.</span></p><p class="c14 c12"><span class="c2">PUBMED: PubMed ID of publications reporting functional evidence of the variant.</span></p><p class="c14 c12"><span class="c2">MOTIF_NAME: name of the DNA motif affected by the variant.</span></p><p class="c14 c12"><span class="c2">MOTIF_POS: position of the variant within the affected DNA motif.</span></p><p class="c14 c12"><span class="c2">HIGH_INF_POS: indication of whether the variant falls in a highly conserved position within the DNA motif.</span></p><p class="c14 c12"><span class="c2">MOTIF_SCORE_CHANGE: the effect of the variant on the score of the affected DNA motif.</span></p><p class="c14 c12"><span class="c2">TRANSCRIPTION_FACTORS: transcription factors that bind the affected DNA motif.</span></p><p class="c14 c12"><span class="c2">CADD_PHRED: Phred-scaled CADD score (Combined Annotation-Dependent Depletion), which predicts the deleteriousness of variants.</span></p><p class="c14 c12"><span class="c2">CADD_RAW: the raw CADD score, which is a measure of the deleteriousness of variants.</span></p><p class="c14 c12"><span class="c2">GO Terms: Gene Ontology (GO) terms associated with the affected gene.</span></p><p class="c14 c12"><span class="c2">The VEP file provides detailed information about the functional and ontological consequences of genetic variants, including their impact on genes, proteins, and pathways.</span></p><p class="c14 c12"><span class="c2">We have converted the text file to a tsv file and trimmed down the file to include only the rsID, Alleles, CADD_PHRED and CADD_RAW scores columns for the functional data and the rsID, location, gene, symbol, GO terms columns for the Ontology data. We have further used these TSV files in our database.</span></p><p class="c14 c12"><span class="c2">CADD (Combined Annotation Dependent Depletion) is a tool used for predicting the potential harm caused by genetic variants. The tool generates a score that indicates the likelihood of a variant being deleterious.</span></p><p class="c14 c12"><span class="c2">One advantage of CADD over SIFT and PolyPhen is that CADD integrates a larger and more diverse set of functional annotations. It also considers the effects of variants on non-coding regions of the genome, which can be important for understanding the functional consequences of variants that are not in protein-coding regions.</span></p><p class="c9 c12 c19"><span class="c2"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c8 c32" id="h.bob8u1u023ib"><span class="c10"></span></h3><h3 class="c8" id="h.x88z4oadkpbf"><span class="c10">Linkage Disequilibrium:</span></h3><p class="c14"><span>Linkage disequilibrium (LD) is the degree of non-random association of the allele of one SNP with the allele of another SNP within a population. LD is typically measured by two metrics: D&rsquo; and r</span><span class="c16">2</span><span class="c2">. </span></p><p class="c14"><span class="c2">D&rsquo; is the normalised values of D, the coefficient of linkage disequilibrium, where A and B are alleles of two SNPs in different loci:</span></p><p class="c47"><img src="images/image1.png"><span class="c39 c42">&nbsp;</span></p><p class="c14"><img src="images/image2.png"><span class="c16">&nbsp; </span><span class="c2">is the correlation coefficient between two loci:</span></p><p class="c47"><img src="images/image3.png"></p><h3 class="c45" id="h.9g8mzey51un2"><span class="c41 c39">Collecting data</span></h3><p class="c14"><span>Linkage disequilibrium data was obtained from LDlink using the LDmatrix tool. D&rsquo; and r</span><span class="c16">2</span><span>&nbsp;values for SNPs were calculated using 1000 Genomes Project data for all three populations. LD data was obtained by inputting a list of SNPs from the same chromosome and selecting the population which would be used for allele frequency data for LD calculations. LDmatrix would produce two text files containing a matrix of results for D&rsquo; and r</span><span class="c16">2</span><span class="c2">&nbsp;values calculated between all SNPs pair combinations in the input list. This was performed separately for each population. Some SNPs did not have any LD data due to a lack of allele frequency data for those SNPs in the 1000 Genomes Project.</span></p><p class="c9"><span>LD datasets containing D&rsquo; and r</span><span class="c16">2</span><span class="c2">&nbsp;values for Finnish, Toscani and British populations are loaded in with pandas as separate dataframes. Each dataframe has their index set to the first column which contains SNP rsIDs.</span></p><a id="t.2d68cf246bdeee4863dcdfbc42860528c30c8e2e"></a><a id="t.8"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c24">import</span><span class="c1">&nbsp;pandas </span><span class="c24">as</span><span class="c1">&nbsp;pd<br></span><span class="c6"># Finland (FIN)</span><span class="c1"><br>LD_D_FIN = pd.read_table(</span><span class="c26">&#39;FIN_D.txt&#39;</span><span class="c1">)<br>LD_D_FIN = LD_D_FIN.set_index(</span><span class="c26">&#39;RS_number&#39;</span><span class="c1">)<br>LD_r2_FIN = pd.read_table(</span><span class="c26">&#39;FIN_r2.txt&#39;</span><span class="c1">)<br>LD_r2_FIN = LD_r2_FIN.set_index(</span><span class="c26">&#39;RS_number&#39;</span><span class="c1">)<br></span><span class="c6"># Italy - Tuscany (TSI)</span><span class="c1"><br>LD_D_TSI = pd.read_table(</span><span class="c26">&#39;TSI_D.txt&#39;</span><span class="c1">)<br>LD_D_TSI = LD_D_TSI.set_index(</span><span class="c26">&#39;RS_number&#39;</span><span class="c1">)<br>LD_r2_TSI = pd.read_table(</span><span class="c26">&#39;TSI_r2.txt&#39;</span><span class="c1">)<br>LD_r2_TSI = LD_r2_TSI.set_index(</span><span class="c26">&#39;RS_number&#39;</span><span class="c1">)<br></span><span class="c6"># British (GBR)</span><span class="c1"><br>LD_D_GBR = pd.read_table(</span><span class="c26">&#39;GBR_D.txt&#39;</span><span class="c1">)<br>LD_D_GBR = LD_D_GBR.set_index(</span><span class="c26">&#39;RS_number&#39;</span><span class="c1">)<br>LD_r2_GBR = pd.read_table(</span><span class="c26">&#39;GBR_r2.txt&#39;</span><span class="c1">)<br>LD_r2_GBR = LD_r2_GBR.set_index(</span><span class="c26">&#39;RS_number&#39;</span><span class="c1">)</span></p></td></tr></table><p class="c9 c19"><span class="c17"></span></p><p class="c9"><span class="c2">This function uses the itertools combination function to create a list of tuples containing all unique pairs of SNPs possible from a list of SNPs. The list is then separated into two lists containing the first and second element of each tuple.</span></p><a id="t.3cd888f33c1969e34d6e88ae1b5769c14c614333"></a><a id="t.9"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c6"># Take list of SNPs and creates a pair of lists containing the 1st and 2nd SNP of each combination &nbsp;</span><span class="c1"><br></span><span class="c24">def</span><span class="c1">&nbsp;</span><span class="c40">SNP_pair_lists</span><span class="c1">(SNP_list):<br> &nbsp; &nbsp;SNP_combinations = list(itertools.combinations(SNP_list,</span><span class="c7">2</span><span class="c1">))<br> &nbsp; &nbsp;SNP_1_list = []<br> &nbsp; &nbsp;SNP_2_list = []<br> &nbsp; &nbsp;</span><span class="c24">for</span><span class="c1">&nbsp;SNP_pair </span><span class="c24">in</span><span class="c1">&nbsp;SNP_combinations:<br> &nbsp; &nbsp; &nbsp; &nbsp;SNP_1_list.append(SNP_pair[</span><span class="c7">0</span><span class="c1">])<br> &nbsp; &nbsp; &nbsp; &nbsp;SNP_2_list.append(SNP_pair[</span><span class="c7">1</span><span class="c1">])<br> &nbsp; &nbsp;</span><span class="c24">return</span><span class="c1">&nbsp;SNP_1_list, SNP_2_list<br><br>SNP_1_list, SNP_2_list = SNP_pair_lists(SNP_list)</span></p></td></tr></table><p class="c9 c19"><span class="c17"></span></p><p class="c9"><span class="c2">An empty dataframe is created to be filled with rows containing data from all six dataframes. This loop uses the two lists created from the SNP list to index each dataframe and extract the respective LD value. These are used to create a list which is converted into a single row pandas dataframe which is added to the empty dataframe using pandas concat until data for all relevant pairwise LD calculations have been added. The completed dataframe is then outputted as a TSV file.</span></p><a id="t.a0d21dcccdcbe0c4a56237beb3046f132352f165"></a><a id="t.10"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c6"># Create Empty dataframe </span><span class="c1"><br>LD_dataset = pd.DataFrame(columns=[</span><span class="c26">&#39;SNP_1&#39;</span><span class="c1">, </span><span class="c26">&#39;SNP_2&#39;</span><span class="c1">, </span><span class="c26">&#39;FIN_D\&#39;&#39;</span><span class="c1">, </span><span class="c26">&#39;FIN_r2&#39;</span><span class="c1">, </span><span class="c26">&#39;TSI_D\&#39;&#39;</span><span class="c1">, </span><span class="c26">&#39;TSI_r2&#39;</span><span class="c1">, </span><span class="c26">&#39;GBR_D\&#39;&#39;</span><span class="c1">, </span><span class="c26">&#39;GBR_r2&#39;</span><span class="c1">])<br></span><span class="c6"># Indexes the respective LD calculation for each pair and adds it to the data</span><span class="c1"><br></span><span class="c24">for</span><span class="c1">&nbsp;SNP_1,SNP_2 </span><span class="c24">in</span><span class="c1">&nbsp;zip(SNP_1_list,SNP_2_list):<br> &nbsp; &nbsp;</span><span class="c6"># Finland</span><span class="c1"><br> &nbsp; &nbsp;FIN_D = LD_D_FIN[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;FIN_r2 = LD_r2_FIN[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;</span><span class="c6"># Italy - Tuscany</span><span class="c1"><br> &nbsp; &nbsp;TSI_D = LD_D_TSI[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;TSI_r2 = LD_r2_TSI[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;</span><span class="c6"># British</span><span class="c1"><br> &nbsp; &nbsp;GBR_D = LD_D_GBR[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;GBR_r2 = LD_r2_GBR[SNP_1].loc[SNP_2]<br> &nbsp; &nbsp;</span><span class="c6"># Create row of data and combine with LD dataset dataframe</span><span class="c1"><br> &nbsp; &nbsp;row_list = [SNP_1, SNP_2, FIN_D, FIN_r2, TSI_D, TSI_r2, GBR_D, GBR_r2]<br> &nbsp; &nbsp;row = pd.DataFrame(row_list).T<br> &nbsp; &nbsp;row.columns = LD_dataset.columns<br> &nbsp; &nbsp;LD_dataset = pd.concat([LD_dataset, row])<br></span><span class="c6"># Write out LD dataset as a TSV</span><span class="c1"><br>LD_dataset.to_csv(</span><span class="c26">&#39;LD_T1DM_Chr6.tsv&#39;</span><span class="c1">, sep=</span><span class="c26">&quot;\t&quot;</span><span class="c1">, index=</span><span class="c24">False</span><span class="c1">)</span></p></td></tr></table><p class="c9 c19"><span class="c17"></span></p><p class="c9 c19"><span class="c17"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c27" id="h.czvflz3vh9cw"><span class="c41 c39"></span></h3><h3 class="c37 c31" id="h.2wm5ip20i7c1"><span class="c39 c41">Outputting LD results:</span></h3><p class="c9"><span class="c2">When a user searches by gene name or chromosomal coordinates, if multiple SNPs are returned, a list of SNPs is used to filter the LD dataset for all rows with entries for all pairwise LD calculations of SNPs in the list and output a results dataframe.</span></p><p class="c9 c19"><span class="c2"></span></p><p class="c9"><span class="c2">Before filtering, the list is checked for any SNPs which are not in the LD dataset due to lack of LD data and any offending SNPs are removed from the list. </span></p><a id="t.2485981c42700898c52a2bdcff2edf8fc12f566c"></a><a id="t.11"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c24">def</span><span class="c1">&nbsp;</span><span class="c40">remove_invalid_SNPs</span><span class="c1">(SNP_list, LD_dataset_file = </span><span class="c26">&quot;data/TSVs/LD_T1DM_Chr6.tsv&quot;</span><span class="c1">):<br></span><span class="c6"># remove SNPs returned from query which have no LD values in LD dataset</span><span class="c1"><br> &nbsp; &nbsp;</span><span class="c6"># Load LD dataset as pandas dataframe</span><span class="c1"><br> &nbsp; &nbsp;LD_df = pd.read_table(LD_dataset_file)<br> &nbsp; &nbsp;</span><span class="c6"># checks for SNPs in subset which are not in LD dataset</span><span class="c1"><br> &nbsp; &nbsp;invalid_list = []<br> &nbsp; &nbsp;</span><span class="c24">for</span><span class="c1">&nbsp;SNP </span><span class="c24">in</span><span class="c1">&nbsp;SNP_list:<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24">if</span><span class="c1">&nbsp;SNP </span><span class="c24">not</span><span class="c1">&nbsp;</span><span class="c24">in</span><span class="c1">&nbsp;LD_df[</span><span class="c26">&#39;SNP_1&#39;</span><span class="c1">].tolist(): </span><span class="c6"># check if SNP is in LD dataset</span><span class="c1"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;invalid_list.append(SNP) </span><span class="c6"># add to list of invalid SNPs</span><span class="c1"><br> &nbsp; &nbsp;print(invalid_list)<br> &nbsp; &nbsp;</span><span class="c6"># remove invalid SNPs from SNP list passed to LD plot</span><span class="c1"><br> &nbsp; &nbsp;</span><span class="c24">for</span><span class="c1">&nbsp;SNP </span><span class="c24">in</span><span class="c1">&nbsp;invalid_list:<br> &nbsp; &nbsp; &nbsp; &nbsp;SNP_list.remove(SNP)<br> &nbsp; &nbsp;</span><span class="c24">return</span><span class="c1 c39">&nbsp;SNP_list<br><br>SNP_list = remove_invalid_SNPs(SNP_list)</span></p></td></tr></table><p class="c9 c19"><span class="c2"></span></p><p class="c9"><span class="c2">The SNP list is then used to create two lists containing the first and second element of each tuple using the SNP_pair_lists() function defined earlier.</span></p><a id="t.467397c9bfe0b58ffeebec61aad4b21b644362e1"></a><a id="t.12"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c6"># create a pair of lists containing the 1st and 2nd SNP of each combination</span><span class="c1"><br>SNP_1_list, SNP_2_list = SNP_pair_lists(SNP_list)</span></p></td></tr></table><p class="c9 c19"><span class="c2"></span></p><p class="c9"><span class="c2">The LD dataset containing all available data for pairwise LD calculations is loaded in with pandas and an empty dataframe is created for the filtered data. The pair of SNP lists are then used to index the LD dataset dataframe for all rows with pairs of SNPs relevant to the user&rsquo;s search query which are added to the LD results dataframe using pandas concat.</span></p><a id="t.258f382ba68c7d1d3a17e1bbd7a0ff9175b6a7fa"></a><a id="t.13"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c6"># Load LD dataset and create empty dataframe for filtered results</span><span class="c1"><br>LD_df = pd.read_table(LD_dataset_file)<br>LD_results_df = pd.DataFrame(columns=[</span><span class="c26">&#39;SNP_1&#39;</span><span class="c1">, </span><span class="c26">&#39;SNP_2&#39;</span><span class="c1">, </span><span class="c26">&#39;FIN_D\&#39;&#39;</span><span class="c1">, </span><span class="c26">&#39;FIN_r2&#39;</span><span class="c1">, </span><span class="c26">&#39;TSI_D\&#39;&#39;</span><span class="c1">, </span><span class="c26">&#39;TSI_r2&#39;</span><span class="c1">, </span><span class="c26">&#39;GBR_D\&#39;&#39;</span><span class="c1">, </span><span class="c26">&#39;GBR_r2&#39;</span><span class="c1">]) <br></span><span class="c6"># Loop indexing LD dataset using each pair of SNPs</span><span class="c1"><br></span><span class="c24">for</span><span class="c1">&nbsp;SNP_1,SNP_2 </span><span class="c24">in</span><span class="c1">&nbsp;zip(SNP_1_list,SNP_2_list):<br> &nbsp; &nbsp;LD_row = LD_df.loc[((LD_df[</span><span class="c26">&#39;SNP_1&#39;</span><span class="c1">] == SNP_1) &amp; (LD_df[</span><span class="c26">&#39;SNP_2&#39;</span><span class="c1">] == SNP_2) | <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(LD_df[</span><span class="c26">&#39;SNP_1&#39;</span><span class="c1">] == SNP_2) &amp; (LD_df[</span><span class="c26">&#39;SNP_2&#39;</span><span class="c1">] == SNP_1))] <br> &nbsp; &nbsp;LD_results_df = pd.concat([LD_results_df, LD_row])</span></p></td></tr></table><p class="c9 c19"><span class="c17"></span></p><hr style="page-break-before:always;display:none;"><h3 class="c27" id="h.tm8mndqz55x9"><span class="c41 c39"></span></h3><h3 class="c31 c37" id="h.hebu2brlzmob"><span class="c41 c39">LD heatmap plots:</span></h3><p class="c14 c12"><span>When a user searches by gene name or chromosomal coordinates, if multiple SNPs are returned, a list of SNPs is also used to extract LD values for all relevant pairwise SNP calculations to create a dataframe used to create a heatmap plot of LD values.</span></p><p class="c9"><span class="c2">The LD dataset is loaded in with pandas and SNPs not present in the dataset are removed from the list of SNPs passed from the user query. The SNP list is then used to create two lists containing the first and second element of each tuple using the SNP_pair_lists() function defined earlier.</span></p><a id="t.37287ca283631a102efe72ff61cf4610e2d4e853"></a><a id="t.14"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c6"># load LD dataset</span><span class="c1"><br>LD_df = pd.read_table(</span><span class="c26">&#39;LD_T1DM_Chr6.tsv&#39;</span><span class="c1">)<br></span><span class="c6"># checks for SNPs in subset which are not in LD dataset</span><span class="c1 c39"><br>SNP_list = remove_invalid_SNPs(SNP_list)</span></p><p class="c9"><span class="c6"># create a pair of lists containing the 1st and 2nd SNP of each combination</span><span class="c1 c39"><br>SNP_1_list, SNP_2_list = SNP_pair_lists(SNP_list)</span></p></td></tr></table><p class="c9 c19"><span class="c17"></span></p><p class="c9"><span class="c2">An empty dataframe is created to be filled with LD values used to create the LD plot. The pair of SNP lists are then used to index the LD dataset dataframe and extract the LD value for all possible pairwise LD calculations from the SNP list. A row of LD values is created for each SNP where each column corresponds with the pairwise LD calculation with one of the SNPs from the list. Each row is added to the empty dataframe using pandas concat.</span></p><a id="t.f562ae5cedba9ec0d08c9ebb9c8653d95d6fa722"></a><a id="t.15"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c1">LD_matrix_df = pd.DataFrame(columns=[SNP_list]) </span><span class="c6"># One column per SNP in list (Since a list object is passed, could just pass the SNP list</span><span class="c1"><br></span><span class="c24">for</span><span class="c1">&nbsp;SNP_1 </span><span class="c24">in</span><span class="c1">&nbsp;SNP_list:<br> &nbsp; &nbsp;</span><span class="c6"># Create empty list</span><span class="c1"><br> &nbsp; &nbsp;LD_value_list = []<br> &nbsp; &nbsp;</span><span class="c6"># Sub-loop - Loops to create list of datapoints</span><span class="c1"><br> &nbsp; &nbsp;</span><span class="c24">for</span><span class="c1">&nbsp;SNP_2 </span><span class="c24">in</span><span class="c1">&nbsp;SNP_list:<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24">if</span><span class="c1">&nbsp;SNP_1 == SNP_2:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SNP_Datapoint = </span><span class="c7">1</span><span class="c1"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LD_value_list.append(SNP_Datapoint)<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c24">else</span><span class="c1">:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c6">#try:</span><span class="c1"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c6"># Search for specific row containing value</span><span class="c1"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LD_row = LD_df.loc[((LD_df[</span><span class="c26">&#39;SNP_1&#39;</span><span class="c1">] == SNP_1) &amp; (LD_df[</span><span class="c26">&#39;SNP_2&#39;</span><span class="c1">] == SNP_2) | <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(LD_df[</span><span class="c26">&#39;SNP_1&#39;</span><span class="c1">] == SNP_2) &amp; (LD_df[</span><span class="c26">&#39;SNP_2&#39;</span><span class="c1">] == SNP_1))] <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c6"># Extract value and add to list</span><span class="c1"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;SNP_Datapoint = LD_row[</span><span class="c26">&#39;GBR_r2&#39;</span><span class="c1">].tolist()[</span><span class="c7">0</span><span class="c1">] </span><span class="c6"># currently using Finnish data</span><span class="c1"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;LD_value_list.append(SNP_Datapoint)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c6">#except:</span><span class="c1"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c6">#invalid_list.append((SNP_main,SNP_second))</span><span class="c1"><br> &nbsp; &nbsp;</span><span class="c6"># Convert into dataframe row and transpose</span><span class="c1"><br> &nbsp; &nbsp;row = pd.DataFrame(LD_value_list).T<br> &nbsp; &nbsp;row.columns = LD_matrix_df.columns<br> &nbsp; &nbsp;LD_matrix_df = pd.concat([LD_matrix_df, row])</span></p></td></tr></table><p class="c9 c19"><span class="c17"></span></p><p class="c9"><span class="c2">The LD matrix dataframe is passed to the ld_plot function. The number of rows (n) is used to create a mask which will hide half of the heatmap to create a triangular plot. A coordinate matrix is also created to rotate the heatmap plot. The SNP list is used to create the axis labels located at the bottom of the plot. The function&rsquo;s title parameter passes a string which is used to determine the plot title.</span></p><a id="t.29a54725cd80635d2188dacbe57f49bc64d7b5b3"></a><a id="t.16"></a><table class="c22"><tr class="c25"><td class="c20" colspan="1" rowspan="1"><p class="c9"><span class="c24">def</span><span class="c1">&nbsp;</span><span class="c40">ld_plot</span><span class="c1">(ld, labels, title):<br> &nbsp; &nbsp;n = ld.shape[</span><span class="c7">0</span><span class="c1">]<br><br> &nbsp; &nbsp;figure = plt.figure()<br><br> &nbsp; &nbsp;</span><span class="c6"># mask triangle matrix</span><span class="c1"><br> &nbsp; &nbsp;mask = np.tri(n, k=</span><span class="c7">0</span><span class="c1">)<br> &nbsp; &nbsp;ld_masked = np.ma.array(ld, mask=mask)<br><br> &nbsp; &nbsp;</span><span class="c6"># create rotation/scaling matrix</span><span class="c1"><br> &nbsp; &nbsp;t = np.array([[</span><span class="c7">1</span><span class="c1">, </span><span class="c7">0.5</span><span class="c1">], [</span><span class="c7">-1</span><span class="c1">, </span><span class="c7">0.5</span><span class="c1">]])<br> &nbsp; &nbsp;</span><span class="c6"># create coordinate matrix and transform it</span><span class="c1"><br> &nbsp; &nbsp;coordinate_matrix = np.dot(np.array([(i[</span><span class="c7">1</span><span class="c1">], i[</span><span class="c7">0</span><span class="c1">])<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c24">for</span><span class="c1">&nbsp;i </span><span class="c24">in</span><span class="c1">&nbsp;itertools.product(range(n, </span><span class="c7">-1</span><span class="c1">, </span><span class="c7">-1</span><span class="c1">), range(</span><span class="c7">0</span><span class="c1">, n + </span><span class="c7">1</span><span class="c1">, </span><span class="c7">1</span><span class="c1">))]), t)<br> &nbsp; &nbsp;</span><span class="c6"># plot</span><span class="c1"><br> &nbsp; &nbsp;ax = figure.add_subplot(</span><span class="c7">1</span><span class="c1">, </span><span class="c7">1</span><span class="c1">, </span><span class="c7">1</span><span class="c1">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c26">&#39;bottom&#39;</span><span class="c1">].set_position(</span><span class="c26">&#39;center&#39;</span><span class="c1">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c26">&#39;top&#39;</span><span class="c1">].set_visible(</span><span class="c24">False</span><span class="c1">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c26">&#39;right&#39;</span><span class="c1">].set_visible(</span><span class="c24">False</span><span class="c1">)<br> &nbsp; &nbsp;ax.spines[</span><span class="c26">&#39;left&#39;</span><span class="c1">].set_visible(</span><span class="c24">False</span><span class="c1">)<br> &nbsp; &nbsp;ax.get_yaxis().set_visible(</span><span class="c24">False</span><span class="c1">)<br> &nbsp; &nbsp;plt.tick_params(axis=</span><span class="c26">&#39;x&#39;</span><span class="c1">, which=</span><span class="c26">&#39;both&#39;</span><span class="c1">, top=</span><span class="c24">False</span><span class="c1">)<br> &nbsp; &nbsp;plt.pcolor(coordinate_matrix[:, </span><span class="c7">1</span><span class="c1">].reshape(n + </span><span class="c7">1</span><span class="c1">, n + </span><span class="c7">1</span><span class="c1">),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; coordinate_matrix[:, </span><span class="c7">0</span><span class="c1">].reshape(n + </span><span class="c7">1</span><span class="c1">, n + </span><span class="c7">1</span><span class="c1">), <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; np.flipud(ld_masked), edgecolors = </span><span class="c26">&quot;white&quot;</span><span class="c1">, <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; linewidth = </span><span class="c7">1.5</span><span class="c1">, cmap = </span><span class="c26">&#39;OrRd&#39;</span><span class="c1">)<br> &nbsp; &nbsp;plt.xticks(ticks=np.arange(len(labels)) + </span><span class="c7">0.5</span><span class="c1">, labels=labels, rotation=</span><span class="c26">&#39;vertical&#39;</span><span class="c1">, fontsize=</span><span class="c7">8</span><span class="c1">)<br> &nbsp; &nbsp;plt.colorbar()<br><br> &nbsp; &nbsp;</span><span class="c6"># add title</span><span class="c1"><br> &nbsp; &nbsp;plt.title(</span><span class="c26">f&quot;{title}&quot;</span><span class="c1">, loc = </span><span class="c26">&quot;center&quot;</span><span class="c1">)<br> &nbsp; <br> &nbsp; &nbsp;</span><span class="c24">return</span><span class="c1">&nbsp;figure<br><br>LD_heatmap_plot = ld_plot(LD_matrix_df, SNP_list, </span><span class="c26">&quot;LD plot title&quot;</span><span class="c1 c39">)</span></p></td></tr></table><p class="c9 c19"><span class="c17"></span></p><p class="c9 c12 c19"><span class="c2"></span></p><p class="c9 c12 c19"><span class="c2"></span></p><h3 class="c8" id="h.5zfnv0ig9q0m"><span class="c10">Manhattan Plot:</span></h3><h2 class="c9 c12 c31" id="h.hwsiupgp3f8i"><span class="c30">Flask:</span></h2><p class="c9 c12 c19"><span class="c2"></span></p><h2 class="c9 c12 c31" id="h.147jjkfr6rv5"><span class="c30">Navigation:</span></h2><p class="c9 c12 c19"><span class="c2"></span></p><h2 class="c9 c12 c31" id="h.fysf9tqgmmsh"><span class="c30">Citation:</span></h2><p class="c9 c12 c19"><span class="c2"></span></p><p class="c9 c12 c19"><span class="c2"></span></p><h3 class="c8 c32" id="h.nrbww7njcv3p"><span class="c10"></span></h3><p class="c9 c12 c19"><span class="c2"></span></p><p class="c9 c12 c19"><span class="c2"></span></p><p class="c9 c12 c19"><span class="c2"></span></p><h1 class="c12 c31 c46" id="h.1yu3kvn20cvs"><span class="c49"></span></h1></body></html>